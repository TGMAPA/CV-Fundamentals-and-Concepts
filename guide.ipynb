{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e906137",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Computer Vision Exam Guide \n",
    "Miguel Ángel Pérez Ávila\n",
    "\n",
    "---\n",
    "\n",
    "# Imágenes como Datos Numéricos\n",
    "\n",
    "## 1. Visión por Computadora (Computer Vision)\n",
    "\n",
    "### Definición\n",
    "La visión por computadora es un campo que permite a las máquinas interpretar y comprender el mundo visual mediante algoritmos y modelos que procesan, analizan y extraen información significativa de imágenes y videos.\n",
    "\n",
    "**Objetivo:** Replicar las capacidades de visión humana para identificar e interpretar objetos visuales, escenas y actividades.\n",
    "\n",
    "### Aplicaciones Principales\n",
    "\n",
    "1. **Vehículos Autónomos**: Reconocimiento de señales, peatones y obstáculos\n",
    "2. **Imágenes Médicas**: Diagnóstico de enfermedades mediante X-rays, MRIs\n",
    "3. **Retail y E-commerce**: Búsqueda visual y reconocimiento de productos\n",
    "4. **Robótica**: Manipulación de objetos y navegación autónoma\n",
    "5. **Agricultura**: Monitoreo de cultivos y detección de enfermedades\n",
    "6. **Ciberseguridad**: Detección de amenazas, reconocimiento facial y deepfakes\n",
    "\n",
    "## 2. Pipeline de Visión por Computadora\n",
    "\n",
    "### Pasos del Pipeline\n",
    "1. **Adquisición de Imágenes**: Captura de datos visuales\n",
    "2. **Procesamiento**: Estandarización de datos\n",
    "3. **Análisis y Reconocimiento**: Extracción de características\n",
    "4. **Acción**: Toma de decisiones basada en el análisis\n",
    "\n",
    "### Importancia del Pre-procesamiento\n",
    "- Estandariza imágenes a un tamaño uniforme\n",
    "- Normaliza el formato y la escala\n",
    "- Mejora la precisión de clasificación\n",
    "\n",
    "**Ejemplo:** Clasificación de señales de tráfico mediante conteo de píxeles rojos.\n",
    "\n",
    "## 3. Imágenes como Grillas de Píxeles\n",
    "\n",
    "### Conceptos Fundamentales\n",
    "\n",
    "#### Píxel\n",
    "- Unidad básica de una imagen\n",
    "- Representa color o intensidad de luz\n",
    "- Una imagen es una grilla de píxeles\n",
    "\n",
    "**Ejemplo:** Imagen de 500×300 = 150,000 píxeles totales\n",
    "\n",
    "### Tipos de Representación\n",
    "\n",
    "#### Escala de Grises (Grayscale)\n",
    "- Rango de valores: **0-255**\n",
    "- 0 = Negro\n",
    "- 255 = Blanco\n",
    "- Valores intermedios = tonos de gris\n",
    "- Representación: entero de 8 bits sin signo\n",
    "\n",
    "#### Color RGB\n",
    "- Tres componentes: **Red, Green, Blue**\n",
    "- Cada componente: rango 0-255\n",
    "- Formato de tupla: `(red, green, blue)`\n",
    "\n",
    "**Ejemplos:**\n",
    "- Blanco: `(255, 255, 255)`\n",
    "- Negro: `(0, 0, 0)`\n",
    "- Rojo puro: `(255, 0, 0)`\n",
    "\n",
    "### ⚠️ Importante: OpenCV y el Orden BGR\n",
    "OpenCV almacena los canales en orden **BGR** (Blue, Green, Red) en lugar de RGB.\n",
    "\n",
    "## 4. Operaciones con OpenCV y NumPy\n",
    "\n",
    "### Lectura y Conversión de Imágenes\n",
    "\n",
    "```python\n",
    "# Leer imagen\n",
    "image = cv2.imread('ruta/imagen.jpg')\n",
    "\n",
    "# Convertir BGR a RGB\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convertir a escala de grises\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "```\n",
    "\n",
    "### Dimensiones de Imagen\n",
    "- **Color RGB**: `(altura, ancho, 3)` - 3 canales\n",
    "- **Escala de grises**: `(altura, ancho)` - 1 canal\n",
    "\n",
    "### Acceso a Píxeles\n",
    "\n",
    "```python\n",
    "# Acceder a píxel específico\n",
    "pixel = image[y, x]  # Nota: y primero, luego x\n",
    "\n",
    "# Acceder a región\n",
    "region = image[y1:y2, x1:x2]\n",
    "```\n",
    "\n",
    "### Operaciones con Arrays\n",
    "\n",
    "```python\n",
    "# Valores máximo y mínimo\n",
    "max_val = np.amax(gray_image)\n",
    "min_val = np.amin(gray_image)\n",
    "\n",
    "# Modificar región\n",
    "image[y1:y2, x1:x2] = (0, 255, 0)  # Verde\n",
    "```\n",
    "\n",
    "## 5. Creación de Imágenes desde Arrays NumPy\n",
    "\n",
    "### Imagen en Escala de Grises\n",
    "```python\n",
    "tiny_image = np.array([[0, 20, 30],\n",
    "                       [200, 200, 250],\n",
    "                       [50, 180, 85]])\n",
    "```\n",
    "\n",
    "### Imagen RGB\n",
    "```python\n",
    "# Crear imagen de tamaño específico con 3 canales\n",
    "gradient = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "```\n",
    "\n",
    "## 6. Conceptos Clave para Recordar\n",
    "\n",
    "- **Píxel**: Unidad básica de información visual\n",
    "- **Coordenadas**: `[y, x]` en arrays de NumPy (fila, columna)\n",
    "- **Rango de valores**: 0-255 (8 bits)\n",
    "- **BGR vs RGB**: OpenCV usa BGR por defecto\n",
    "- **Slicing**: Permite manipular regiones de la imagen\n",
    "- **dtype=np.uint8**: Tipo de dato para imágenes (0-255)\n",
    "\n",
    "## 7. Técnicas de Visualización\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mostrar imagen RGB\n",
    "plt.imshow(image_rgb)\n",
    "\n",
    "# Mostrar imagen en escala de grises\n",
    "plt.imshow(gray_image, cmap='gray')\n",
    "\n",
    "# Múltiples subplots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(image1)\n",
    "ax2.imshow(image2)\n",
    "```\n",
    "\n",
    "## 8. Ejercicio Práctico: Gradiente\n",
    "\n",
    "**Objetivo**: Crear una imagen con gradiente de rojo a blanco.\n",
    "\n",
    "**Concepto**: Incrementar progresivamente los valores de los canales Green y Blue mientras Red permanece en 255.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 9. Masking (Enmascaramiento)\n",
    "\n",
    "### Definición\n",
    "El masking permite enfocarse únicamente en las porciones de una imagen que son de interés, ignorando el resto del contenido. Es una técnica fundamental para segmentación y procesamiento selectivo de imágenes.\n",
    "\n",
    "**Aplicación Principal**: Extraer o aislar regiones específicas de una imagen basándose en criterios de color, intensidad u otras características.\n",
    "\n",
    "### Concepto de Máscara\n",
    "- Una **máscara** es una imagen binaria (valores 0 y 255) del mismo tamaño que la imagen original\n",
    "- Píxeles con valor **255** (blancos) indican regiones de interés\n",
    "- Píxeles con valor **0** (negros) indican regiones a ignorar\n",
    "\n",
    "### Umbral de Color (Color Threshold)\n",
    "\n",
    "#### Definir Rangos de Color\n",
    "Para crear una máscara basada en color, se definen límites inferior y superior en RGB:\n",
    "\n",
    "```python\n",
    "# Definir rango de color (ejemplo: azul)\n",
    "lower_blue = np.array([0, 0, 180])\n",
    "upper_blue = np.array([70, 70, 255])\n",
    "```\n",
    "\n",
    "#### Crear Máscara con inRange\n",
    "```python\n",
    "# cv2.inRange verifica si los píxeles están dentro del rango especificado\n",
    "mask = cv2.inRange(image_rgb, lower_blue, upper_blue)\n",
    "\n",
    "# Visualizar la máscara\n",
    "plt.imshow(mask, cmap='gray')\n",
    "```\n",
    "\n",
    "### Aplicar Máscaras a Imágenes\n",
    "\n",
    "#### Método 1: Filtrado Booleano\n",
    "```python\n",
    "# Crear copia de la imagen\n",
    "masked_image = np.copy(image_rgb)\n",
    "\n",
    "# Crear filtro booleano\n",
    "filter = mask != 0\n",
    "\n",
    "# Aplicar filtro (píxeles que coinciden con la máscara se vuelven negros)\n",
    "masked_image[filter] = [0, 0, 0]\n",
    "```\n",
    "\n",
    "#### Método 2: Operaciones Bitwise\n",
    "```python\n",
    "# Invertir máscara\n",
    "mask_inverted = cv2.bitwise_not(mask)\n",
    "\n",
    "# Aplicar máscara usando operación AND\n",
    "masked_image = cv2.bitwise_and(image_rgb, image_rgb, mask=mask_inverted)\n",
    "```\n",
    "\n",
    "### Operaciones Bitwise Importantes\n",
    "\n",
    "| Operación | Función | Uso |\n",
    "|-----------|---------|-----|\n",
    "| `cv2.bitwise_not()` | Invierte la máscara | Intercambiar regiones de interés |\n",
    "| `cv2.bitwise_and()` | AND lógico | Aplicar máscara a imagen |\n",
    "| `cv2.bitwise_or()` | OR lógico | Combinar máscaras |\n",
    "| `cv2.bitwise_xor()` | XOR lógico | Diferencia entre máscaras |\n",
    "\n",
    "### Caso Práctico: Cambio de Fondo (Blue Screen)\n",
    "\n",
    "#### Proceso Completo\n",
    "1. **Crear máscara** del fondo a eliminar (ej. fondo azul)\n",
    "2. **Aislar objeto** eliminando el fondo\n",
    "3. **Preparar nuevo fondo** con las mismas dimensiones\n",
    "4. **Combinar** objeto y nuevo fondo\n",
    "\n",
    "```python\n",
    "# 1. Crear máscara del fondo azul\n",
    "mask = cv2.inRange(image_rgb, lower_blue, upper_blue)\n",
    "\n",
    "# 2. Aislar objeto (pizza)\n",
    "masked_object = np.copy(image_rgb)\n",
    "masked_object[mask != 0] = [0, 0, 0]\n",
    "\n",
    "# 3. Preparar nuevo fondo\n",
    "background = cv2.imread('nuevo_fondo.jpg')\n",
    "background = cv2.cvtColor(background, cv2.COLOR_BGR2RGB)\n",
    "crop_background = background[0:height, 0:width]\n",
    "\n",
    "# Bloquear área del objeto en el fondo\n",
    "crop_background[mask == 0] = [0, 0, 0]\n",
    "\n",
    "# 4. Combinar objeto y fondo\n",
    "complete_image = crop_background + masked_object\n",
    "```\n",
    "\n",
    "### Máscaras Multicanal\n",
    "\n",
    "Para trabajar con máscaras RGB (3 canales), se puede usar `np.all()` para crear filtros:\n",
    "\n",
    "```python\n",
    "# Definir color específico\n",
    "target_color = np.array([255, 0, 255])  # Magenta\n",
    "\n",
    "# Crear filtro que verifique los 3 canales\n",
    "filter = np.all(image == target_color, axis=-1)\n",
    "\n",
    "# Aplicar filtro\n",
    "result = np.zeros_like(image)\n",
    "result[filter] = [255, 255, 255]  # Píxeles coincidentes en blanco\n",
    "```\n",
    "\n",
    "### Segmentación por Máscaras de Clase\n",
    "\n",
    "En aplicaciones de segmentación semántica, diferentes clases se representan con colores únicos:\n",
    "\n",
    "| Clase | R | G | B | Descripción |\n",
    "|-------|---|---|---|-------------|\n",
    "| Terreno urbano | 0 | 255 | 255 | Áreas construidas |\n",
    "| Agricultura | 255 | 255 | 0 | Cultivos |\n",
    "| Pastizal | 255 | 0 | 255 | Vegetación baja |\n",
    "| Bosque | 0 | 255 | 0 | Áreas forestales |\n",
    "| Agua | 0 | 0 | 255 | Cuerpos de agua |\n",
    "| Tierra árida | 255 | 255 | 255 | Suelo sin vegetación |\n",
    "| Desconocido | 0 | 0 | 0 | Sin clasificar |\n",
    "\n",
    "### Consejos Prácticos\n",
    "\n",
    "1. **Redimensionar imágenes**: Asegurar que todas las imágenes tengan las mismas dimensiones\n",
    "    ```python\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    ```\n",
    "\n",
    "2. **Tipo de datos**: Las máscaras deben ser `dtype=np.uint8`\n",
    "\n",
    "3. **Visualización**: Usar `cmap='gray'` para mostrar máscaras binarias correctamente\n",
    "\n",
    "4. **Ajuste de umbrales**: Experimentar con los rangos de color hasta aislar correctamente la región de interés\n",
    "\n",
    "### Conceptos Clave\n",
    "\n",
    "- **Máscara binaria**: Imagen de 2 valores (0 y 255)\n",
    "- **inRange**: Función para detectar píxeles dentro de un rango de color\n",
    "- **Operaciones bitwise**: Permiten combinar y manipular máscaras\n",
    "- **Filtrado booleano**: Usar condiciones para seleccionar píxeles\n",
    "- **np.all()**: Verificar condiciones en múltiples canales simultáneamente\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 10. Separación de Canales de Color (Color Channel Splitting)\n",
    "\n",
    "### Definición\n",
    "La separación de canales de color es el proceso de descomponer una imagen RGB en sus componentes individuales (Rojo, Verde, Azul) para analizarlos y manipularlos de forma independiente.\n",
    "\n",
    "**Aplicación Principal**: Análisis individual de componentes de color, extracción de características específicas por canal, y filtrado selectivo.\n",
    "\n",
    "### Métodos de Separación de Canales\n",
    "\n",
    "#### Método 1: Indexación con NumPy\n",
    "```python\n",
    "# Separar canales RGB mediante indexación\n",
    "r = image[:, :, 0]  # Canal Rojo\n",
    "g = image[:, :, 1]  # Canal Verde\n",
    "b = image[:, :, 2]  # Canal Azul\n",
    "```\n",
    "\n",
    "#### Método 2: cv2.split()\n",
    "```python\n",
    "# Separar canales usando OpenCV\n",
    "(r, g, b) = cv2.split(image)\n",
    "```\n",
    "\n",
    "**Ventajas de cv2.split()**: Más eficiente y retorna una tupla con los canales separados.\n",
    "\n",
    "### Visualización de Canales\n",
    "\n",
    "#### Visualización en Escala de Grises\n",
    "Muestra la intensidad de cada canal independientemente:\n",
    "\n",
    "```python\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "ax1.set_title('Canal R')\n",
    "ax1.imshow(r, cmap='gray')\n",
    "\n",
    "ax2.set_title('Canal G')\n",
    "ax2.imshow(g, cmap='gray')\n",
    "\n",
    "ax3.set_title('Canal B')\n",
    "ax3.imshow(b, cmap='gray')\n",
    "```\n",
    "\n",
    "**Interpretación**: Píxeles más brillantes indican mayor presencia de ese color.\n",
    "\n",
    "#### Visualización con Colores Específicos\n",
    "Muestra cada canal con su color correspondiente:\n",
    "\n",
    "```python\n",
    "names = ['R Channel', 'G Channel', 'B Channel']\n",
    "figure, plots = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "for i, subplot, name in zip(range(3), plots, names):\n",
    "    temp = np.zeros(image.shape, dtype='uint8')\n",
    "    temp[:, :, i] = image[:, :, i]\n",
    "    subplot.set_title(name)\n",
    "    subplot.imshow(temp)\n",
    "```\n",
    "\n",
    "**Concepto**: Crea una imagen temporal con ceros en todos los canales excepto el que se quiere visualizar.\n",
    "\n",
    "### Combinación de Canales con cv2.merge()\n",
    "\n",
    "```python\n",
    "# Combinar canales RGB en una imagen\n",
    "image_merged = cv2.merge((r, g, b))\n",
    "```\n",
    "\n",
    "**Uso**: Reconstruir la imagen original después de manipular canales individuales o crear imágenes sintéticas combinando diferentes canales.\n",
    "\n",
    "### Aplicaciones Prácticas\n",
    "\n",
    "1. **Análisis de dominancia de color**: Identificar qué canal tiene mayor intensidad\n",
    "2. **Corrección de color**: Ajustar canales individuales para balancear la imagen\n",
    "3. **Extracción de características**: Aislar objetos basándose en su predominancia en un canal específico\n",
    "4. **Efectos artísticos**: Manipular canales para crear efectos visuales\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Espacios de Color (Color Spaces)\n",
    "\n",
    "### Definición\n",
    "Un espacio de color es un modelo matemático que representa los colores como tuplas de números (usualmente 3 o 4 valores). Define el rango de colores que un dispositivo digital puede capturar, mostrar o imprimir.\n",
    "\n",
    "**Propósito**: Representar y manipular colores de manera consistente y predecible.\n",
    "\n",
    "### Espacios de Color Principales\n",
    "\n",
    "#### RGB (Red, Green, Blue)\n",
    "- Modelo aditivo de color\n",
    "- Basado en la mezcla de luz\n",
    "- Coordenadas cartesianas (cubo)\n",
    "- Rango: 0-255 por canal\n",
    "\n",
    "**Uso**: Pantallas, cámaras digitales, procesamiento general de imágenes\n",
    "\n",
    "#### HSV (Hue, Saturation, Value)\n",
    "- **Hue (Matiz)**: Tipo de color (0-179° en OpenCV)\n",
    "- **Saturation (Saturación)**: Pureza del color (0-255)\n",
    "- **Value (Valor)**: Brillo del color (0-255)\n",
    "\n",
    "**Geometría**: Representación cilíndrica del espacio RGB\n",
    "\n",
    "#### HSL (Hue, Saturation, Lightness)\n",
    "Similar a HSV pero con Lightness en lugar de Value:\n",
    "- **Lightness**: Nivel de iluminación del color\n",
    "\n",
    "### Conversión entre Espacios de Color\n",
    "\n",
    "```python\n",
    "# Convertir RGB a HSV\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Convertir RGB a HSL\n",
    "hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "\n",
    "# Convertir de vuelta a RGB\n",
    "rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "```\n",
    "\n",
    "### Análisis de Canales HSV\n",
    "\n",
    "```python\n",
    "# Separar canales HSV\n",
    "h = hsv[:, :, 0]  # Hue (Matiz)\n",
    "s = hsv[:, :, 1]  # Saturation (Saturación)\n",
    "v = hsv[:, :, 2]  # Value (Valor)\n",
    "\n",
    "# Verificar rangos\n",
    "print(f'Min - Max hue: {np.min(h)} - {np.max(h)}')\n",
    "print(f'Min - Max saturation: {np.min(s)} - {np.max(s)}')\n",
    "print(f'Min - Max value: {np.min(v)} - {np.max(v)}')\n",
    "\n",
    "# Visualizar canales\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "ax1.imshow(h, cmap='gray')\n",
    "ax2.imshow(s, cmap='gray')\n",
    "ax3.imshow(v, cmap='gray')\n",
    "```\n",
    "\n",
    "### Ventajas de HSV sobre RGB\n",
    "\n",
    "1. **Segmentación de color más intuitiva**: Separar el color del brillo\n",
    "2. **Invariancia a la iluminación**: El canal Hue es más robusto a cambios de iluminación\n",
    "3. **Selección de rangos más natural**: Más fácil definir umbrales de color\n",
    "\n",
    "### Caso Práctico: Eliminación de Fondo Verde (Green Screen)\n",
    "\n",
    "```python\n",
    "# Leer imagen con fondo verde\n",
    "image = cv2.imread('car_green_screen.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convertir a HSV para mejor segmentación de color\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Definir rango para color verde en HSV\n",
    "lower_green = np.array([40, 40, 40])\n",
    "upper_green = np.array([80, 255, 255])\n",
    "\n",
    "# Crear máscara del fondo verde\n",
    "mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "# Invertir máscara para obtener solo el objeto\n",
    "mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "# Extraer objeto sin fondo\n",
    "object_only = cv2.bitwise_and(image, image, mask=mask_inv)\n",
    "```\n",
    "\n",
    "### Rangos de Hue en OpenCV\n",
    "\n",
    "| Color | Rango Hue (OpenCV) |\n",
    "|-------|-------------------|\n",
    "| Rojo | 0-10, 170-180 |\n",
    "| Naranja | 10-25 |\n",
    "| Amarillo | 25-35 |\n",
    "| Verde | 35-85 |\n",
    "| Cian | 85-95 |\n",
    "| Azul | 95-130 |\n",
    "| Violeta | 130-160 |\n",
    "| Magenta | 160-170 |\n",
    "\n",
    "**⚠️ Nota**: OpenCV usa rango 0-179 para Hue (no 0-360 como en definiciones estándar).\n",
    "\n",
    "### Comparación RGB vs HSV para Masking\n",
    "\n",
    "**RGB**:\n",
    "- Requiere definir rangos en tres canales simultáneamente\n",
    "- Sensible a variaciones de iluminación\n",
    "- Más complejo para aislar colores específicos\n",
    "\n",
    "**HSV**:\n",
    "- Más intuitivo para selección de colores (usar principalmente canal H)\n",
    "- Robusto ante cambios de iluminación\n",
    "- Ideal para segmentación basada en color\n",
    "\n",
    "### Conceptos Clave\n",
    "\n",
    "- **Espacio de color**: Modelo matemático para representar colores\n",
    "- **HSV**: Modelo cilíndrico más intuitivo para segmentación\n",
    "- **Hue**: Representa el tipo de color independiente del brillo\n",
    "- **cv2.cvtColor()**: Función para convertir entre espacios de color\n",
    "- **Invariancia a iluminación**: HSV separa color de brillo\n",
    "\n",
    "### Consejos Prácticos\n",
    "\n",
    "1. **Usar HSV para masking**: Más efectivo que RGB para segmentación de color\n",
    "2. **Experimentar con rangos**: Los valores óptimos dependen de condiciones de iluminación\n",
    "3. **Visualizar canales**: Ayuda a entender qué canal usar para cada tarea\n",
    "4. **Normalización**: Algunos espacios de color requieren normalización (0-1) para ciertos algoritmos\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Filtros en Procesamiento de Imágenes\n",
    "\n",
    "### Definición de Ruido\n",
    "El ruido en una imagen se refiere a variaciones aleatorias o no deseadas en la intensidad de los píxeles que oscurecen la información visual real. El ruido típicamente se origina de:\n",
    "- Condiciones ambientales\n",
    "- Limitaciones del sensor\n",
    "- Errores de transmisión\n",
    "\n",
    "**Impacto**: Puede degradar significativamente la calidad de la imagen y afectar el resultado de pasos posteriores de procesamiento.\n",
    "\n",
    "### ¿Qué son los Filtros?\n",
    "Los filtros son algoritmos o técnicas aplicados a imágenes para mejorar o modificar sus propiedades visuales. Son herramientas fundamentales para:\n",
    "- Reducción de ruido\n",
    "- Detección de bordes\n",
    "- Desenfoque (blurring)\n",
    "- Enfoque (sharpening)\n",
    "- Extracción de características\n",
    "\n",
    "**Principio**: Filtran información no deseada o irrelevante, o amplifican características como límites de objetos.\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Frecuencia en Imágenes\n",
    "\n",
    "### Definición\n",
    "La frecuencia en imágenes es la **tasa de cambio**: la velocidad a la que cambian los valores de intensidad de los píxeles a través de una imagen.\n",
    "\n",
    "### Componentes de Frecuencia\n",
    "\n",
    "#### 1. Componentes de Baja Frecuencia (Low-Frequency)\n",
    "Corresponden a cambios lentos o graduales en la intensidad de píxeles:\n",
    "\n",
    "**Características**:\n",
    "- Gradientes suaves\n",
    "- Áreas uniformes grandes\n",
    "- Estructura general y formas básicas\n",
    "\n",
    "**Ejemplos**:\n",
    "- Cielo despejado\n",
    "- Objetos de color sólido\n",
    "- Fondos uniformes\n",
    "\n",
    "#### 2. Componentes de Alta Frecuencia (High-Frequency)\n",
    "Refieren a cambios rápidos en la intensidad de píxeles:\n",
    "\n",
    "**Características**:\n",
    "- Bordes afilados\n",
    "- Texturas finas\n",
    "- Detalles pequeños\n",
    "\n",
    "**Ejemplos**:\n",
    "- Límites entre objetos distintos\n",
    "- Texturas de tela\n",
    "- Detalles finos\n",
    "\n",
    "**Visualización**: En una imagen de alta frecuencia, la intensidad cambia mucho y rápidamente entre píxeles adyacentes.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Convolución (Convolution)\n",
    "\n",
    "### Definición Matemática\n",
    "La convolución es una técnica fundamental en procesamiento de imágenes que combina dos funciones.\n",
    "\n",
    "#### Formulación Continua\n",
    "Para funciones continuas f(x) y g(x):\n",
    "\n",
    "$$f(x) * g(x) = \\int_{-\\infty}^{\\infty} f(u) g(x-u) du$$\n",
    "\n",
    "#### Formulación Discreta (Imágenes Digitales)\n",
    "Para imágenes digitales bidimensionales:\n",
    "\n",
    "$$F(x, y) = \\sum_i \\sum_j f(x+i, y+j) h(i, j)$$\n",
    "\n",
    "### Algoritmo para Vecindario 3×3\n",
    "\n",
    "Para un kernel (máscara) 3×3:\n",
    "\n",
    "$$\\begin{bmatrix} h_1 & h_2 & h_3 \\\\ h_4 & h_5 & h_6 \\\\ h_7 & h_8 & h_9 \\end{bmatrix}$$\n",
    "\n",
    "```python\n",
    "# Pseudocódigo\n",
    "for all pixels in image do {\n",
    "    Q0 = P0*h0 + P1*h1 + P2*h2 + P3*h3 + P4*h4 + \n",
    "         P5*h5 + P6*h6 + P7*h7 + P8*h8;\n",
    "}\n",
    "```\n",
    "\n",
    "**Concepto**: La convolución aplica una función de dispersión de punto (kernel) a todos los puntos de una imagen y acumula las contribuciones.\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Kernels de Convolución\n",
    "\n",
    "### Definición de Kernel\n",
    "Un **kernel** (núcleo o máscara) es una matriz de números que modifica una imagen mediante convolución.\n",
    "\n",
    "### Kernel Laplaciano (Detección de Bordes)\n",
    "\n",
    "$$\\begin{pmatrix} 0 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 0 \\end{pmatrix}$$\n",
    "\n",
    "**Características**:\n",
    "- Todos los elementos suman cero\n",
    "- Calcula la diferencia o cambio entre píxeles vecinos\n",
    "- Sustrae los valores de los píxeles circundantes del píxel central\n",
    "\n",
    "### Proceso de Aplicación\n",
    "1. El kernel pasa sobre la imagen píxel por píxel\n",
    "2. En cada posición, se realiza la operación de convolución\n",
    "3. El resultado transforma la imagen basándose en los valores del kernel\n",
    "\n",
    "### Implementación con OpenCV\n",
    "\n",
    "```python\n",
    "# Definir kernel personalizado\n",
    "sobel_y = np.array([[-1, -2, -1],\n",
    "                    [ 0,  0,  0],\n",
    "                    [ 1,  2,  1]])\n",
    "\n",
    "# Aplicar filtro usando cv2.filter2D\n",
    "# Parámetros: (imagen en escala de grises, profundidad de bits, kernel)\n",
    "filtered_image = cv2.filter2D(gray_image, -1, sobel_y)\n",
    "```\n",
    "**Parámetro -1**: Indica que la imagen de salida tendrá la misma profundidad que la imagen de entrada.\n",
    "\n",
    "\n",
    "![Convolution](https://raw.githubusercontent.com/octavio-navarro/Computer-Vision/main/Notebooks/images/Convolution.png)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 16. Filtros Paso-Alto (High-Pass Filters)\n",
    "\n",
    "### Propósito\n",
    "Los filtros paso-alto se usan para:\n",
    "- Hacer que una imagen aparezca más nítida\n",
    "- Realzar partes de alta frecuencia de una imagen\n",
    "- Enfatizar cambios rápidos entre píxeles vecinos\n",
    "\n",
    "### Funcionamiento\n",
    "Operan sobre imágenes en escala de grises para:\n",
    "- Detectar patrones de intensidad\n",
    "- Resaltar áreas donde un píxel es mucho más brillante que sus vecinos\n",
    "- Crear líneas que enfatizan estos cambios\n",
    "\n",
    "### Detección de Bordes\n",
    "**Bordes**: Áreas en una imagen donde la intensidad cambia muy rápidamente, indicando límites de objetos.\n",
    "\n",
    "**Resultado**: Los filtros paso-alto realzan estos bordes, creando representaciones que destacan los contornos de objetos.\n",
    "\n",
    "### Operadores Sobel\n",
    "\n",
    "#### Sobel X (Detección de bordes verticales)\n",
    "$$\\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "#### Sobel Y (Detección de bordes horizontales)\n",
    "$$\\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}$$\n",
    "\n",
    "```python\n",
    "# Aplicar operadores Sobel\n",
    "sobel_x = np.array([[-1, 0, 1],\n",
    "                    [-2, 0, 2],\n",
    "                    [-1, 0, 1]])\n",
    "\n",
    "sobel_y = np.array([[-1, -2, -1],\n",
    "                    [ 0,  0,  0],\n",
    "                    [ 1,  2,  1]])\n",
    "\n",
    "filtered_x = cv2.filter2D(gray_image, -1, sobel_x)\n",
    "filtered_y = cv2.filter2D(gray_image, -1, sobel_y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 17. Imágenes Binarias y Umbralización (Thresholding)\n",
    "\n",
    "### Objetivo de Segmentación\n",
    "Separar regiones oscuras y claras de una imagen para identificar objetos en fondos contrastantes.\n",
    "\n",
    "### Umbralización (Thresholding)\n",
    "\n",
    "```python\n",
    "# cv2.threshold(imagen_fuente, valor_umbral, valor_máximo, tipo_umbral)\n",
    "retval, binary_image = cv2.threshold(filtered_image, 10, 255, cv2.THRESH_BINARY)\n",
    "```\n",
    "\n",
    "**Parámetros**:\n",
    "- **Valor umbral (10)**: Píxeles ≤ 10 → 0 (negro); píxeles > 10 → 255 (blanco)\n",
    "- **Valor máximo (255)**: Valor asignado a píxeles que superan el umbral\n",
    "- **Tipo**: `cv2.THRESH_BINARY` para binarización simple\n",
    "\n",
    "### Métodos Adaptativos\n",
    "- **Otsu**: Calcula automáticamente el umbral óptimo\n",
    "- **Triangle**: Método del triángulo para histogramas bimodales\n",
    "\n",
    "**retval**: Almacena el valor del umbral calculado cuando se usan métodos adaptativos.\n",
    "\n",
    "---\n",
    "\n",
    "## 18. Filtros Paso-Bajo (Low-Pass Filters)\n",
    "\n",
    "### Propósito\n",
    "Eliminar ruido de las imágenes mediante:\n",
    "- Bloqueo de contenido de alta frecuencia\n",
    "- Suavizado o desenfoque de la apariencia de la imagen\n",
    "- Reducción de ruido de alta frecuencia\n",
    "\n",
    "### Importancia\n",
    "El ruido puede afectar negativamente pasos posteriores de procesamiento. Los filtros paso-alto pueden amplificar el ruido si no se elimina primero.\n",
    "\n",
    "### Kernel de Promediado (Averaging Kernel)\n",
    "\n",
    "El filtro paso-bajo más simple:\n",
    "\n",
    "$$\\frac{1}{9} \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix}$$\n",
    "\n",
    "**Funcionamiento**: Toma un promedio de los píxeles vecinos (no una diferencia como los filtros paso-alto).\n",
    "\n",
    "---\n",
    "\n",
    "## 19. Filtro Gaussiano (Gaussian Blur)\n",
    "\n",
    "### Características\n",
    "El filtro más frecuentemente usado en visión por computadora:\n",
    "- Desenfoca la imagen\n",
    "- Preserva información de bordes\n",
    "- Es esencialmente un promedio ponderado\n",
    "\n",
    "### Kernel Gaussiano 3×3\n",
    "\n",
    "$$\\frac{1}{16} \\begin{pmatrix} 1 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 1 \\end{pmatrix}$$\n",
    "\n",
    "**Ponderación**: Da más peso al píxel central, considerando también los píxeles circundantes.\n",
    "\n",
    "### Implementación\n",
    "\n",
    "```python\n",
    "# Aplicar desenfoque gaussiano\n",
    "# El kernel debe tener dimensiones impares para centrarse en cada píxel\n",
    "gray_blur = cv2.GaussianBlur(gray_image, (3, 3), 0)\n",
    "```\n",
    "\n",
    "**Parámetros**:\n",
    "- `(3, 3)`: Tamaño del kernel (debe ser impar)\n",
    "- `0`: Desviación estándar (0 = calculada automáticamente)\n",
    "\n",
    "### Aplicaciones\n",
    "- **Imágenes médicas**: Reducir ruido excesivo en resonancias magnéticas\n",
    "- **Pre-procesamiento**: Antes de aplicar filtros de detección de bordes\n",
    "- **Mejora de calidad**: Suavizar imágenes ruidosas\n",
    "\n",
    "---\n",
    "\n",
    "## 20. Detección de Bordes Canny\n",
    "\n",
    "### Definición\n",
    "El detector de bordes Canny es uno de los mejores y más utilizados algoritmos de detección de bordes. Considera múltiples aspectos:\n",
    "- Nivel de cambio de intensidad que constituye un borde\n",
    "- Detección consistente de bordes finos y gruesos\n",
    "- Producción de bordes de un píxel de grosor\n",
    "\n",
    "### Proceso de Canny (4 Pasos)\n",
    "\n",
    "#### 1. Filtrado de Ruido\n",
    "Aplica desenfoque gaussiano para eliminar ruido.\n",
    "\n",
    "#### 2. Cálculo de Gradiente\n",
    "Encuentra la intensidad y dirección de bordes usando filtros Sobel.\n",
    "\n",
    "#### 3. Supresión No-Máxima (Non-Maximum Suppression)\n",
    "- Examina la intensidad y dirección de cada borde detectado\n",
    "- Selecciona el píxel de máximo local\n",
    "- Crea líneas delgadas consistentes de un píxel de grosor alineadas con los bordes más fuertes\n",
    "\n",
    "#### 4. Umbralización por Histéresis (Hysteresis Thresholding)\n",
    "Aísla los mejores bordes usando dos umbrales:\n",
    "\n",
    "- **Umbral alto**: Bordes fuertes (definitivamente bordes)\n",
    "- **Umbral bajo**: Bordes débiles (candidatos)\n",
    "- **Proceso**: Conecta bordes débiles a bordes fuertes si están conectados\n",
    "\n",
    "### Implementación\n",
    "\n",
    "```python\n",
    "# Aplicar detector Canny\n",
    "edges = cv2.Canny(gray_image, umbral_bajo, umbral_alto)\n",
    "\n",
    "# Ejemplos con diferentes umbrales\n",
    "edges_1 = cv2.Canny(gray_image, 0, 20)      # Muchos bordes\n",
    "edges_2 = cv2.Canny(gray_image, 50, 100)    # Bordes moderados\n",
    "edges_3 = cv2.Canny(gray_image, 200, 240)   # Pocos bordes fuertes\n",
    "```\n",
    "\n",
    "**Parámetros críticos**:\n",
    "- **Umbral bajo**: Valor mínimo de intensidad de gradiente\n",
    "- **Umbral alto**: Valor que define bordes fuertes\n",
    "- **Relación recomendada**: Alto = 2-3 × Bajo\n",
    "\n",
    "### Ventajas\n",
    "- Detecta límites con precisión\n",
    "- Produce imagen binaria con bordes bien definidos\n",
    "- Útil para seleccionar áreas de interés para enmascaramiento o análisis posterior\n",
    "\n",
    "---\n",
    "\n",
    "## 21. Filtros Adicionales\n",
    "\n",
    "### Filtro de Enfoque (Sharpen)\n",
    "\n",
    "$$\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix}$$\n",
    "\n",
    "**Efecto**: Aumenta el contraste entre píxeles vecinos, haciendo la imagen más nítida.\n",
    "\n",
    "### Filtro de Relieve (Emboss)\n",
    "\n",
    "$$\\begin{bmatrix} -2 & -1 & 0 \\\\ -1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}$$\n",
    "\n",
    "**Efecto**: Crea una apariencia de relieve 3D, resaltando cambios de intensidad direccionales.\n",
    "\n",
    "### Filtro de Contorno (Outline)\n",
    "\n",
    "$$\\begin{bmatrix} -1 & -1 & -1 \\\\ -1 & 8 & -1 \\\\ -1 & -1 & -1 \\end{bmatrix}$$\n",
    "\n",
    "**Efecto**: Resalta los contornos de objetos, similar a la detección de bordes.\n",
    "\n",
    "### Aplicación de Filtros Personalizados\n",
    "\n",
    "```python\n",
    "# Definir filtros\n",
    "sharpen_filter = np.array([[0, -1, 0],\n",
    "                           [-1, 5, -1],\n",
    "                           [0, -1, 0]])\n",
    "\n",
    "emboss_filter = np.array([[-2, -1, 0],\n",
    "                          [-1, 1, 1],\n",
    "                          [0, 1, 2]])\n",
    "\n",
    "outline_filter = np.array([[-1, -1, -1],\n",
    "                           [-1, 8, -1],\n",
    "                           [-1, -1, -1]])\n",
    "\n",
    "# Aplicar filtros\n",
    "sharpen = cv2.filter2D(image, -1, sharpen_filter)\n",
    "emboss = cv2.filter2D(image, -1, emboss_filter)\n",
    "outline = cv2.filter2D(image, -1, outline_filter)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 22. Conceptos Clave de Filtros\n",
    "\n",
    "### Resumen de Filtros\n",
    "\n",
    "| Tipo de Filtro | Propósito | Frecuencia Afectada |\n",
    "|----------------|-----------|---------------------|\n",
    "| **Paso-Bajo** | Reducir ruido, suavizar | Atenúa alta frecuencia |\n",
    "| **Paso-Alto** | Detección de bordes, enfoque | Realza alta frecuencia |\n",
    "| **Gaussiano** | Suavizado preservando bordes | Atenúa alta frecuencia |\n",
    "| **Canny** | Detección óptima de bordes | Detecta cambios rápidos |\n",
    "\n",
    "### Consideraciones Importantes\n",
    "\n",
    "1. **Orden de operaciones**: Generalmente se aplica filtro paso-bajo (reducción de ruido) antes de paso-alto (detección de bordes)\n",
    "\n",
    "2. **Tamaño de kernel**: \n",
    "   - Kernels más grandes → mayor suavizado/desenfoque\n",
    "   - Debe ser impar para tener píxel central\n",
    "\n",
    "3. **Selección de umbrales**:\n",
    "   - Experimentar con diferentes valores\n",
    "   - Depende de las condiciones de iluminación y contenido de la imagen\n",
    "\n",
    "4. **Suma de elementos del kernel**:\n",
    "   - Suma = 0: Filtros de detección de cambios (bordes)\n",
    "   - Suma = 1: Filtros que preservan brillo promedio\n",
    "\n",
    "### Aplicaciones Prácticas\n",
    "\n",
    "- **Preprocesamiento**: Limpiar imágenes antes de análisis\n",
    "- **Segmentación**: Separar objetos del fondo\n",
    "- **Extracción de características**: Identificar contornos y formas\n",
    "- **Mejora de calidad**: Reducir artefactos y ruido\n",
    "- **Análisis médico**: Procesar imágenes de diagnóstico (MRI, rayos X)\n",
    "\n",
    "### Consejos Prácticos\n",
    "\n",
    "1. **Visualizar resultados intermedios**: Ayuda a entender el efecto de cada filtro\n",
    "2. **Combinar filtros**: Usar múltiples filtros en secuencia para mejores resultados\n",
    "3. **Ajustar parámetros**: Los valores óptimos varían según la imagen\n",
    "4. **Considerar el ruido**: Siempre evaluar si es necesario filtrado paso-bajo primero\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 23. Operaciones Morfológicas (Morphological Operations)\n",
    "\n",
    "### Definición\n",
    "Las operaciones morfológicas son técnicas de procesamiento de imágenes que trabajan sobre la forma de las características dentro de una imagen. Se aplican típicamente sobre imágenes binarias y modifican la estructura de los objetos mediante kernels (elementos estructurantes).\n",
    "\n",
    "**Aplicación Principal**: Procesamiento de imágenes binarias resultantes de detección de bordes, segmentación o umbralización.\n",
    "\n",
    "---\n",
    "\n",
    "## 24. Dilatación (Dilation)\n",
    "\n",
    "### Definición\n",
    "La dilatación **agranda las áreas brillantes** (blancas) de una imagen añadiendo píxeles a los límites percibidos de los objetos.\n",
    "\n",
    "### Funcionamiento\n",
    "- Expande los bordes de regiones blancas\n",
    "- Los objetos se hacen más grandes\n",
    "- Los agujeros dentro de objetos se hacen más pequeños\n",
    "\n",
    "### Implementación\n",
    "\n",
    "```python\n",
    "# Leer imagen en escala de grises\n",
    "letter = cv2.imread(\"images/j.png\", 0)\n",
    "\n",
    "# Crear kernel (elemento estructurante)\n",
    "kernel = np.ones((13, 13), np.uint8)\n",
    "\n",
    "# Aplicar dilatación\n",
    "dilation = cv2.dilate(letter, kernel, iterations=1)\n",
    "\n",
    "# Visualizar resultado\n",
    "plt.imshow(dilation, cmap='gray')\n",
    "```\n",
    "\n",
    "**Parámetros**:\n",
    "- **kernel**: Matriz que define el tamaño y forma de la dilatación\n",
    "- **iterations**: Número de veces que se aplica la operación\n",
    "\n",
    "### Aplicaciones\n",
    "- Rellenar pequeños agujeros en objetos\n",
    "- Conectar componentes cercanos\n",
    "- Hacer objetos más visibles\n",
    "\n",
    "---\n",
    "\n",
    "## 25. Erosión (Erosion)\n",
    "\n",
    "### Definición\n",
    "La erosión **reduce las áreas brillantes** (blancas) de una imagen eliminando píxeles de los límites de los objetos.\n",
    "\n",
    "### Funcionamiento\n",
    "- Contrae los bordes de regiones blancas\n",
    "- Los objetos se hacen más pequeños\n",
    "- Los agujeros dentro de objetos se hacen más grandes\n",
    "- Puede eliminar objetos pequeños\n",
    "\n",
    "### Implementación\n",
    "\n",
    "```python\n",
    "# Aplicar erosión\n",
    "erosion = cv2.erode(letter, kernel, iterations=1)\n",
    "\n",
    "# Visualizar resultado\n",
    "plt.imshow(erosion, cmap='gray')\n",
    "```\n",
    "\n",
    "### Aplicaciones\n",
    "- Eliminar ruido pequeño (píxeles aislados)\n",
    "- Separar objetos conectados ligeramente\n",
    "- Reducir el tamaño de características\n",
    "\n",
    "### Comparación Visual\n",
    "\n",
    "```python\n",
    "# Comparar las tres operaciones\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "ax1.set_title(\"Dilatación\")\n",
    "ax1.imshow(dilation, cmap=\"gray\")\n",
    "\n",
    "ax2.set_title(\"Original\")\n",
    "ax2.imshow(letter, cmap=\"gray\")\n",
    "\n",
    "ax3.set_title(\"Erosión\")\n",
    "ax3.imshow(erosion, cmap=\"gray\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 26. Apertura (Opening)\n",
    "\n",
    "### Definición\n",
    "La apertura es una **combinación de erosión seguida de dilatación**. Es útil para la reducción de ruido.\n",
    "\n",
    "### Proceso\n",
    "1. **Erosión**: Elimina ruido y objetos pequeños\n",
    "2. **Dilatación**: Restaura el tamaño de los objetos principales\n",
    "\n",
    "### Ventajas\n",
    "- El ruido desaparece en la erosión inicial\n",
    "- Los objetos principales recuperan su tamaño aproximado con la dilatación posterior\n",
    "- **El ruido no reaparece** porque fue eliminado en el primer paso\n",
    "\n",
    "### Implementación\n",
    "\n",
    "```python\n",
    "# Leer imagen con ruido\n",
    "letter_noise = cv2.imread(\"images/j_noise.png\", 0)\n",
    "\n",
    "# Crear kernel\n",
    "kernel = np.ones((9, 9), np.uint8)\n",
    "\n",
    "# Aplicar apertura\n",
    "opening = cv2.morphologyEx(letter_noise, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "# Visualizar comparación\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.set_title(\"J con Ruido\")\n",
    "ax1.imshow(letter_noise, cmap=\"gray\")\n",
    "ax2.set_title(\"Apertura (Opening)\")\n",
    "ax2.imshow(opening, cmap=\"gray\")\n",
    "```\n",
    "\n",
    "### Aplicaciones\n",
    "- Eliminación de ruido de fondo\n",
    "- Separación de objetos conectados por ruido\n",
    "- Limpieza de imágenes binarias\n",
    "\n",
    "---\n",
    "\n",
    "## 27. Cierre (Closing)\n",
    "\n",
    "### Definición\n",
    "El cierre es la **combinación inversa**: dilatación seguida de erosión. Es útil para cerrar pequeños agujeros o áreas oscuras dentro de objetos.\n",
    "\n",
    "### Proceso\n",
    "1. **Dilatación**: Cierra agujeros pequeños y conecta regiones cercanas\n",
    "2. **Erosión**: Restaura el tamaño original de los objetos\n",
    "\n",
    "### Ventajas\n",
    "- Los agujeros pequeños se rellenan durante la dilatación\n",
    "- Los objetos recuperan su tamaño aproximado con la erosión posterior\n",
    "- **Los agujeros permanecen cerrados** porque fueron rellenados en el primer paso\n",
    "\n",
    "### Implementación\n",
    "\n",
    "```python\n",
    "# Leer imagen con puntos oscuros\n",
    "letter_dots = cv2.imread(\"images/j_dots.png\", 0)\n",
    "\n",
    "# Crear kernel\n",
    "kernel = np.ones((9, 9), np.uint8)\n",
    "\n",
    "# Aplicar cierre\n",
    "closing = cv2.morphologyEx(letter_dots, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Visualizar comparación\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.set_title(\"J con Puntos\")\n",
    "ax1.imshow(letter_dots, cmap=\"gray\")\n",
    "ax2.set_title(\"Cierre (Closing)\")\n",
    "ax2.imshow(closing, cmap=\"gray\")\n",
    "```\n",
    "\n",
    "### Aplicaciones\n",
    "- Rellenar agujeros pequeños en objetos\n",
    "- Conectar componentes ligeramente separados\n",
    "- Suavizar contornos de objetos\n",
    "\n",
    "---\n",
    "\n",
    "## 28. Comparación de Operaciones Morfológicas\n",
    "\n",
    "### Tabla Comparativa\n",
    "\n",
    "| Operación | Secuencia | Efecto Principal | Uso Principal |\n",
    "|-----------|-----------|------------------|---------------|\n",
    "| **Dilatación** | N/A | Agranda objetos blancos | Conectar componentes |\n",
    "| **Erosión** | N/A | Reduce objetos blancos | Eliminar ruido pequeño |\n",
    "| **Apertura** | Erosión → Dilatación | Elimina ruido externo | Limpieza de fondo |\n",
    "| **Cierre** | Dilatación → Erosión | Rellena agujeros internos | Completar objetos |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 29. Elementos Estructurantes (Kernels)\n",
    "\n",
    "### Importancia del Tamaño del Kernel\n",
    "El tamaño del kernel determina qué tan agresiva es la operación morfológica:\n",
    "\n",
    "**Kernel pequeño** (ej. 3×3):\n",
    "- Cambios sutiles\n",
    "- Preserva detalles finos\n",
    "- Elimina solo ruido muy pequeño\n",
    "\n",
    "**Kernel grande** (ej. 13×13):\n",
    "- Cambios pronunciados\n",
    "- Puede eliminar detalles importantes\n",
    "- Elimina ruido y objetos más grandes\n",
    "\n",
    "### Formas de Kernels\n",
    "\n",
    "```python\n",
    "# Kernel cuadrado (más común)\n",
    "kernel_square = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Kernel rectangular\n",
    "kernel_rect = np.ones((3, 7), np.uint8)\n",
    "\n",
    "# Kernel circular (usando cv2.getStructuringElement)\n",
    "kernel_circle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "# Kernel en forma de cruz\n",
    "kernel_cross = cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5))\n",
    "```\n",
    "\n",
    "### Selección del Kernel Apropiado\n",
    "- **Cuadrado/Rectangular**: Para objetos generales\n",
    "- **Circular**: Para objetos redondeados\n",
    "- **Cruz**: Para preservar esquinas y detalles lineales\n",
    "\n",
    "---\n",
    "\n",
    "## 30. Operaciones Morfológicas Adicionales\n",
    "\n",
    "### Gradiente Morfológico\n",
    "Diferencia entre dilatación y erosión:\n",
    "\n",
    "```python\n",
    "gradient = cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)\n",
    "```\n",
    "\n",
    "**Uso**: Detecta los contornos de objetos.\n",
    "\n",
    "### Top Hat\n",
    "Diferencia entre imagen original y su apertura:\n",
    "\n",
    "```python\n",
    "tophat = cv2.morphologyEx(image, cv2.MORPH_TOPHAT, kernel)\n",
    "```\n",
    "\n",
    "**Uso**: Extrae elementos pequeños más brillantes que el fondo.\n",
    "\n",
    "### Black Hat\n",
    "Diferencia entre cierre de imagen y la imagen original:\n",
    "\n",
    "```python\n",
    "blackhat = cv2.morphologyEx(image, cv2.MORPH_BLACKHAT, kernel)\n",
    "```\n",
    "\n",
    "**Uso**: Extrae elementos pequeños más oscuros que el fondo.\n",
    "\n",
    "---\n",
    "\n",
    "## 31. Conceptos Clave de Operaciones Morfológicas\n",
    "\n",
    "### Principios Fundamentales\n",
    "\n",
    "1. **Imágenes binarias**: Las operaciones morfológicas funcionan mejor en imágenes binarias (blanco y negro)\n",
    "\n",
    "2. **Elemento estructurante**: El kernel define la vecindad y forma de la operación\n",
    "\n",
    "3. **Iteraciones**: Repetir operaciones amplifica su efecto\n",
    "\n",
    "4. **Orden importa**: La secuencia de operaciones (apertura vs cierre) produce resultados diferentes\n",
    "\n",
    "### Consejos Prácticos\n",
    "\n",
    "1. **Preprocesamiento**: Convertir imagen a binaria antes de aplicar operaciones morfológicas\n",
    "    ```python\n",
    "    _, binary = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    ```\n",
    "\n",
    "2. **Experimentar con tamaños**: Probar diferentes tamaños de kernel para encontrar el óptimo\n",
    "\n",
    "3. **Combinar operaciones**: Usar secuencias de operaciones para mejores resultados\n",
    "\n",
    "4. **Visualizar pasos intermedios**: Ayuda a entender el efecto de cada operación\n",
    "\n",
    "### Aplicaciones Prácticas\n",
    "\n",
    "- **Reconocimiento de caracteres (OCR)**: Limpiar texto escaneado\n",
    "- **Análisis de documentos**: Eliminar ruido de escaneos\n",
    "- **Visión médica**: Procesar imágenes de rayos X y MRI\n",
    "- **Inspección industrial**: Detectar defectos en productos\n",
    "- **Segmentación**: Mejorar resultados de detección de objetos\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 32. Histogramas en Procesamiento de Imágenes\n",
    "\n",
    "### Definición\n",
    "Un histograma representa la **distribución de intensidades de píxeles** (ya sea en color o escala de grises) en una imagen. Se visualiza como un gráfico que proporciona una intuición de alto nivel sobre la distribución de intensidades (valores de píxeles).\n",
    "\n",
    "**Propósito**: Analizar y comprender las características de brillo, contraste y distribución de intensidad de una imagen.\n",
    "\n",
    "### Componentes del Histograma\n",
    "\n",
    "#### Bins (Contenedores)\n",
    "- Son \"cestas\" que cuentan el número de entradas con valores dentro del rango del bin\n",
    "- El número de bins determina la granularidad del histograma\n",
    "- **256 bins**: Cuenta cada valor de píxel individualmente (0-255)\n",
    "- **2 bins**: Agrupa píxeles en rangos [0, 128) y [128, 255]\n",
    "\n",
    "#### Ejes del Histograma\n",
    "- **Eje X**: Valores de intensidad (bins) - rango 0-255\n",
    "- **Eje Y**: Frecuencia (número de píxeles con cada intensidad)\n",
    "\n",
    "### Tipos de Histogramas\n",
    "\n",
    "#### Histograma 1D\n",
    "Para arrays unidimensionales o imágenes en escala de grises:\n",
    "\n",
    "```python\n",
    "def histogram_1D(data, bins):\n",
    "    hist_values = [0] * bins\n",
    "    # Iterar sobre los datos y contar valores en cada bin\n",
    "    for value in data:\n",
    "        bin_index = int(value * bins)\n",
    "        hist_values[bin_index] += 1\n",
    "    return hist_values\n",
    "```\n",
    "\n",
    "#### Histograma 2D (Imagen en Escala de Grises)\n",
    "Para imágenes de un solo canal:\n",
    "\n",
    "```python\n",
    "def histogram_2D(image, bins):\n",
    "    hist_values = [0] * bins\n",
    "    h, w = image.shape\n",
    "    \n",
    "    for j in range(h):\n",
    "        for i in range(w):\n",
    "            hist_values[int(image[j, i])] += 1\n",
    "    \n",
    "    return hist_values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 33. Histogramas con OpenCV\n",
    "\n",
    "### Función cv2.calcHist()\n",
    "\n",
    "```python\n",
    "hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "```\n",
    "\n",
    "**Parámetros**:\n",
    "1. **images**: La imagen para calcular el histograma (como lista: `[myImage]`)\n",
    "2. **channels**: Lista de índices de canales\n",
    "   - `[0]`: Canal 0 (escala de grises o primer canal)\n",
    "   - `[0, 1, 2]`: Todos los canales RGB\n",
    "3. **mask**: Máscara opcional para calcular histograma solo en región específica (`None` para toda la imagen)\n",
    "4. **histSize**: Número de bins (lista: `[256]` para 256 bins)\n",
    "5. **ranges**: Rango de valores posibles (normalmente `[0, 256]` para cada canal)\n",
    "\n",
    "### Visualización de Histogramas\n",
    "\n",
    "```python\n",
    "# Histograma de imagen en escala de grises\n",
    "image = cv2.imread('imagen.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(image, cmap='gray')\n",
    "ax2.plot(hist)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 34. Histogramas de Imágenes en Color\n",
    "\n",
    "### Cálculo por Canal\n",
    "\n",
    "```python\n",
    "# Separar canales RGB\n",
    "bins = 256\n",
    "histogram_values_r = histogram_2D(image[:, :, 0], bins)\n",
    "histogram_values_g = histogram_2D(image[:, :, 1], bins)\n",
    "histogram_values_b = histogram_2D(image[:, :, 2], bins)\n",
    "\n",
    "# Visualizar con colores correspondientes\n",
    "plt.plot(histogram_values_r, color=\"red\")\n",
    "plt.plot(histogram_values_g, color=\"green\")\n",
    "plt.plot(histogram_values_b, color=\"blue\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Interpretación de Histogramas de Color\n",
    "\n",
    "**Ejemplo de imagen de playa**:\n",
    "- **Pico agudo en verde (bin ~100)**: Vegetación y árboles oscuros\n",
    "- **Muchos píxeles azules (170-225)**: Cielo azul claro\n",
    "- **Píxeles azules (25-50)**: Océano oscuro\n",
    "\n",
    "### Información del Histograma\n",
    "\n",
    "Al examinar un histograma, puedes obtener información sobre:\n",
    "- **Contraste**: Distribución amplia = alto contraste\n",
    "- **Brillo**: Posición del pico principal\n",
    "- **Distribución de intensidad**: Forma general del histograma\n",
    "\n",
    "---\n",
    "\n",
    "## 35. Aplicaciones de Histogramas\n",
    "\n",
    "### 1. Determinación de Umbrales\n",
    "\n",
    "Los histogramas ayudan a encontrar puntos apropiados para umbralización:\n",
    "\n",
    "```python\n",
    "# Analizar histograma para encontrar valor de umbral\n",
    "hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "\n",
    "# Aplicar umbral basado en análisis del histograma\n",
    "_, image_binary = cv2.threshold(image, 130, 255, cv2.THRESH_BINARY)\n",
    "```\n",
    "\n",
    "**Uso**: Identificar valores que separan diferentes regiones del histograma.\n",
    "\n",
    "### 2. Evaluación de Calidad de Imagen\n",
    "\n",
    "- **Histograma estrecho**: Imagen de bajo contraste\n",
    "- **Histograma amplio**: Imagen de alto contraste\n",
    "- **Picos múltiples**: Diferentes regiones de intensidad bien definidas\n",
    "\n",
    "---\n",
    "\n",
    "## 36. Ecualización de Histograma (Histogram Equalization)\n",
    "\n",
    "### Definición\n",
    "La ecualización de histograma mejora el **contraste de una imagen** mediante el \"estiramiento\" de la distribución de píxeles.\n",
    "\n",
    "**Concepto**: Redistribuye las intensidades de píxeles para que cubran todo el rango disponible (0-255).\n",
    "\n",
    "### Funcionamiento\n",
    "- Toma un histograma con un pico grande en el centro\n",
    "- \"Estira\" el pico hacia las esquinas de la imagen\n",
    "- Mejora el contraste global de la imagen\n",
    "\n",
    "### Cuándo Usar Ecualización de Histograma\n",
    "\n",
    "**Útil para**:\n",
    "- Imágenes con fondos y primer planos ambos oscuros o ambos claros\n",
    "- Imágenes médicas (rayos X, MRI, tomografías)\n",
    "- Imágenes satelitales\n",
    "- Imágenes con bajo contraste general\n",
    "\n",
    "**Limitaciones**:\n",
    "- Puede producir efectos poco realistas en fotografías normales\n",
    "- Mejor para aplicaciones técnicas que para fotografía artística\n",
    "\n",
    "### Implementación Básica\n",
    "\n",
    "```python\n",
    "# Ecualización en imagen de escala de grises\n",
    "image = cv2.imread('imagen.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Aplicar ecualización\n",
    "eq = cv2.equalizeHist(image)\n",
    "\n",
    "# Visualizar comparación\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(image, cmap='gray')\n",
    "ax2.imshow(eq, cmap='gray')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 37. Ecualización de Histograma en Imágenes RGB\n",
    "\n",
    "### Aplicación por Canal\n",
    "\n",
    "```python\n",
    "# Leer imagen en color\n",
    "image = cv2.imread('beach.png')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Separar canales\n",
    "R, G, B = cv2.split(image)\n",
    "\n",
    "# Ecualizar cada canal individualmente\n",
    "eq_R = cv2.equalizeHist(R)\n",
    "eq_G = cv2.equalizeHist(G)\n",
    "eq_B = cv2.equalizeHist(B)\n",
    "\n",
    "# Combinar canales ecualizados\n",
    "image_eq = cv2.merge([eq_R, eq_G, eq_B])\n",
    "```\n",
    "\n",
    "### Ecualización Selectiva\n",
    "\n",
    "Puedes ecualizar solo ciertos canales para efectos específicos:\n",
    "\n",
    "```python\n",
    "# Ecualizar solo el canal verde\n",
    "image_eq_g = cv2.merge([R, eq_G, B])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 38. Ecualización de Histograma en Espacio HSV\n",
    "\n",
    "### Ventajas de HSV para Ecualización\n",
    "\n",
    "La ecualización en espacio HSV es más efectiva que en RGB porque:\n",
    "- Separa información de color (Hue) del brillo (Value)\n",
    "- Permite manipular contraste sin afectar colores\n",
    "- Produce resultados más naturales\n",
    "\n",
    "### Ecualización del Canal Hue (H)\n",
    "\n",
    "**Beneficios**:\n",
    "1. **Reducir dominancia de color**: Elimina tonos dominantes no naturales\n",
    "2. **Mejorar contraste de color**: Hace la gama de colores más uniforme\n",
    "3. **Mejorar discriminación de color**: Hace diferencias sutiles más pronunciadas\n",
    "\n",
    "### Ecualización del Canal Saturación (S)\n",
    "\n",
    "**Beneficios**:\n",
    "1. **Reducir saturación excesiva**: Hace colores vibrantes más naturales\n",
    "2. **Realzar colores sutiles**: Hace diferencias de saturación más visibles\n",
    "3. **Mejorar armonía de color**: Balancea la intensidad de colores\n",
    "\n",
    "### Ecualización del Canal Valor (V)\n",
    "\n",
    "**Beneficios**:\n",
    "1. **Mejorar brillo general**: Corrige iluminación desigual\n",
    "2. **Mejorar contraste**: Distribuye mejor los niveles de brillo\n",
    "3. **Reducir ruido**: Hace los niveles de brillo más consistentes\n",
    "\n",
    "---\n",
    "\n",
    "## 39. Algoritmo de Ecualización de Histograma\n",
    "\n",
    "### Pasos del Algoritmo\n",
    "\n",
    "#### 1. Calcular Histograma Normalizado\n",
    "Normalización dividiendo la frecuencia de cada bin por el número total de píxeles:\n",
    "\n",
    "$$P_x(i) = \\frac{\\text{frecuencia}(i)}{\\text{total\\_pixeles}}$$\n",
    "\n",
    "#### 2. Función de Distribución Acumulativa (CDF)\n",
    "Calcular la suma acumulativa del histograma normalizado:\n",
    "\n",
    "$$\\text{CDF}(j) = \\sum_{i=0}^j P_x(i)$$\n",
    "\n",
    "#### 3. Tabla de Mapeo de Intensidad\n",
    "Mapear nuevas intensidades de píxeles para cada nivel discreto de intensidad i:\n",
    "\n",
    "$$\\text{mapped\\_pixel\\_value}(i) = (L-1) \\times \\text{CDF}(i)$$\n",
    "\n",
    "Donde L = 256 para representación típica de 8 bits.\n",
    "\n",
    "#### 4. Transformar Imagen Original\n",
    "Crear nueva imagen aplicando la tabla de mapeo a cada píxel.\n",
    "\n",
    "### Implementación Manual\n",
    "\n",
    "```python\n",
    "def histogram_equalization(image):\n",
    "    # 1. Calcular histograma\n",
    "    hist, _ = np.histogram(image.flatten(), 256, [0, 256])\n",
    "    \n",
    "    # 2. Normalizar histograma\n",
    "    hist_normalized = hist / hist.sum()\n",
    "    \n",
    "    # 3. Calcular CDF\n",
    "    cdf = hist_normalized.cumsum()\n",
    "    \n",
    "    # 4. Crear tabla de mapeo\n",
    "    lookup_table = np.uint8((256 - 1) * cdf)\n",
    "    \n",
    "    # 5. Aplicar transformación\n",
    "    equalized_image = lookup_table[image]\n",
    "    \n",
    "    return equalized_image\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 40. Análisis de Histogramas\n",
    "\n",
    "### Visualización Comparativa\n",
    "\n",
    "```python\n",
    "# Comparar imagen original y ecualizada con sus histogramas\n",
    "image = cv2.imread('imagen.jpg', 0)\n",
    "eq = cv2.equalizeHist(image)\n",
    "\n",
    "hist_original = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "hist_eq = cv2.calcHist([eq], [0], None, [256], [0, 256])\n",
    "\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "ax1.set_title('Imagen Original')\n",
    "ax1.imshow(image, cmap='gray')\n",
    "\n",
    "ax2.set_title('Histograma Original')\n",
    "ax2.plot(hist_original)\n",
    "\n",
    "ax3.set_title('Imagen Ecualizada')\n",
    "ax3.imshow(eq, cmap='gray')\n",
    "\n",
    "ax4.set_title('Histograma Ecualizado')\n",
    "ax4.plot(hist_eq)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 41. Conceptos Clave de Histogramas\n",
    "\n",
    "### Principios Fundamentales\n",
    "\n",
    "1. **Distribución de intensidad**: El histograma muestra cómo se distribuyen los valores de píxeles\n",
    "\n",
    "2. **Bins**: Determinan la granularidad del análisis de distribución\n",
    "\n",
    "3. **Normalización**: Permite comparar histogramas de imágenes de diferentes tamaños\n",
    "\n",
    "4. **CDF**: La función de distribución acumulativa es clave para ecualización\n",
    "\n",
    "### Análisis Visual\n",
    "\n",
    "**Histograma con pico a la izquierda**:\n",
    "- Imagen oscura\n",
    "- Muchos píxeles de baja intensidad\n",
    "\n",
    "**Histograma con pico a la derecha**:\n",
    "- Imagen brillante\n",
    "- Muchos píxeles de alta intensidad\n",
    "\n",
    "**Histograma centrado**:\n",
    "- Brillo equilibrado\n",
    "- Buena distribución de intensidades\n",
    "\n",
    "**Histograma bimodal**:\n",
    "- Dos regiones distintas\n",
    "- Útil para umbralización\n",
    "\n",
    "### Consejos Prácticos\n",
    "\n",
    "1. **Analizar antes de procesar**: Examinar el histograma antes de aplicar transformaciones\n",
    "\n",
    "2. **Ecualizar selectivamente**: No siempre es necesario ecualizar todos los canales\n",
    "\n",
    "3. **Considerar el contexto**: La ecualización puede no ser apropiada para todas las imágenes\n",
    "\n",
    "4. **Combinar con otras técnicas**: Usar filtros de suavizado antes de ecualizar para reducir ruido\n",
    "\n",
    "5. **Validar resultados**: Comparar histogramas y visualizaciones antes y después\n",
    "\n",
    "### Aplicaciones Prácticas\n",
    "\n",
    "- **Mejora de imágenes médicas**: Resaltar detalles en rayos X y MRI\n",
    "- **Procesamiento de imágenes satelitales**: Mejorar contraste en imágenes de sensores remotos\n",
    "- **Visión nocturna**: Mejorar visibilidad en condiciones de baja luz\n",
    "- **Reconocimiento de patrones**: Pre-procesamiento para algoritmos de detección\n",
    "- **Fotografía forense**: Revelar detalles ocultos en evidencia fotográfica\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 42. PyTorch Lightning: Framework para Deep Learning\n",
    "\n",
    "### Definición\n",
    "PyTorch Lightning es un framework de deep learning construido sobre PyTorch que organiza el código para eliminar código repetitivo (boilerplate) y permitir escalabilidad máxima. Está diseñado para investigadores de IA profesionales e ingenieros de machine learning que necesitan máxima flexibilidad.\n",
    "\n",
    "**Objetivo**: Estructurar código PyTorch de manera profesional, separando la lógica de investigación de la ingeniería de software.\n",
    "\n",
    "### Ventajas de PyTorch Lightning\n",
    "\n",
    "1. **Organización de código**: Estructura clara mediante métodos del ciclo de vida\n",
    "2. **Eliminación de boilerplate**: El framework maneja loops de entrenamiento, backpropagation, optimización\n",
    "3. **Escalabilidad**: Soporte nativo para GPUs, TPUs, entrenamiento distribuido\n",
    "4. **Reproducibilidad**: Facilita la creación de experimentos reproducibles\n",
    "5. **Callbacks y logging**: Sistema integrado para monitoreo y checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 44. LightningModule: Estructura Base\n",
    "\n",
    "### Definición\n",
    "`LightningModule` es la clase base para construir modelos en PyTorch Lightning. Es equivalente a `nn.Module` de PyTorch pero con métodos adicionales del ciclo de vida.\n",
    "\n",
    "**Concepto**: Separa la lógica del modelo de los detalles de entrenamiento, haciendo el código más modular y mantenible.\n",
    "\n",
    "### Métodos Esenciales del Ciclo de Vida\n",
    "\n",
    "#### 1. `__init__()`\n",
    "Inicializa el modelo, define capas y estructuras de datos:\n",
    "\n",
    "```python\n",
    "def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.history = {\n",
    "        'epochs': [],\n",
    "        'loss': []\n",
    "    }\n",
    "```\n",
    "\n",
    "#### 2. `forward()`\n",
    "Define cómo los datos pasan a través del modelo:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    return self.model(x)\n",
    "```\n",
    "\n",
    "**Propósito**: Actúa como mapper entre capas y funciones de activación.\n",
    "\n",
    "#### 3. `training_step()`\n",
    "Define un paso de entrenamiento (obligatorio):\n",
    "\n",
    "```python\n",
    "def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.forward(x)\n",
    "    loss = nn.functional.mse_loss(y_hat, y)\n",
    "    self.log(\"train_loss\", loss)\n",
    "    return loss\n",
    "```\n",
    "\n",
    "**Parámetros**:\n",
    "- `batch`: Tupla de (features, targets) del DataLoader\n",
    "- `batch_idx`: Índice del batch actual\n",
    "\n",
    "#### 4. `configure_optimizers()`\n",
    "Define el optimizador para el modelo (obligatorio):\n",
    "\n",
    "```python\n",
    "def configure_optimizers(self):\n",
    "    optimizer = optim.Adam(self.parameters(), lr=1e-2)\n",
    "    return optimizer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 45. Preparación de Datos con PyTorch\n",
    "\n",
    "### TensorDataset\n",
    "\n",
    "Wrapper para crear datasets a partir de tensores:\n",
    "\n",
    "```python\n",
    "# Crear tensores de entrada y etiquetas\n",
    "and_input = torch.Tensor([[0.,0.], [0.,1.], [1.,0.], [1., 1.]])\n",
    "and_labels = torch.Tensor([[0.],[1.], [1.], [0.]])\n",
    "\n",
    "# Crear dataset que envuelve los tensores\n",
    "and_data = TensorDataset(and_input, and_labels)\n",
    "```\n",
    "\n",
    "**Funcionamiento**: Cada muestra se recupera indexando los tensores a lo largo de la primera dimensión.\n",
    "\n",
    "### DataLoader\n",
    "\n",
    "Combina un dataset con un sampler, proporcionando un iterable sobre el dataset:\n",
    "\n",
    "```python\n",
    "train_loader = DataLoader(and_data, batch_size=4, shuffle=True)\n",
    "\n",
    "# Iterar sobre el dataset\n",
    "for data in train_loader.dataset:\n",
    "    print(data)\n",
    "```\n",
    "\n",
    "**Parámetros importantes**:\n",
    "- `batch_size`: Número de muestras por batch\n",
    "- `shuffle`: Si mezclar los datos en cada epoch\n",
    "- `num_workers`: Procesos para carga de datos en paralelo\n",
    "\n",
    "---\n",
    "\n",
    "## 46. Construcción de Modelos: Operador AND/XOR\n",
    "\n",
    "### Problema XOR\n",
    "El operador XOR (OR exclusivo) es un problema clásico que no puede resolverse con un perceptrón simple. Requiere una red neuronal con capas ocultas para aprender la relación entre entradas y salidas.\n",
    "\n",
    "**Importancia histórica**: Demuestra la necesidad de redes neuronales multicapa.\n",
    "\n",
    "### Arquitectura del Modelo AND\n",
    "\n",
    "```python\n",
    "# Stack de capas para operador AND\n",
    "and_layers = nn.Sequential(\n",
    "    nn.Linear(2, 2),      # Capa de entrada a oculta\n",
    "    nn.Sigmoid(),          # Activación\n",
    "    nn.Linear(2, 1),      # Capa oculta a salida\n",
    "    nn.Sigmoid()           # Activación de salida\n",
    ")\n",
    "\n",
    "# Crear modelo Lightning\n",
    "and_model = ANDModel(and_layers)\n",
    "```\n",
    "\n",
    "**Componentes**:\n",
    "- **Entrada**: 2 neuronas (valores binarios)\n",
    "- **Capa oculta**: 2 neuronas con activación Sigmoid\n",
    "- **Salida**: 1 neurona con activación Sigmoid\n",
    "\n",
    "---\n",
    "\n",
    "## 47. Clase Trainer de PyTorch Lightning\n",
    "\n",
    "### Definición\n",
    "La clase `Trainer` es una abstracción que maneja todo el código boilerplate de entrenamiento:\n",
    "- Loops sobre el dataset\n",
    "- Backpropagation\n",
    "- Limpieza de gradientes\n",
    "- Paso del optimizador\n",
    "\n",
    "### Configuración Básica\n",
    "\n",
    "```python\n",
    "# Crear callback para guardar checkpoints\n",
    "checkpoint_callback = ModelCheckpoint()\n",
    "\n",
    "# Inicializar trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1000,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "```\n",
    "\n",
    "**Parámetros comunes**:\n",
    "- `max_epochs`: Número máximo de épocas\n",
    "- `callbacks`: Lista de callbacks para funcionalidad adicional\n",
    "- `accelerator`: Tipo de hardware ('cpu', 'gpu', 'tpu')\n",
    "- `devices`: Número de dispositivos a usar\n",
    "\n",
    "### Entrenamiento del Modelo\n",
    "\n",
    "```python\n",
    "# Entrenar modelo\n",
    "trainer.fit(model=and_model, train_dataloaders=train_loader)\n",
    "```\n",
    "\n",
    "**Proceso automático**:\n",
    "1. Itera sobre épocas\n",
    "2. Para cada batch: forward pass, cálculo de pérdida, backward pass\n",
    "3. Actualiza pesos con el optimizador\n",
    "4. Registra métricas\n",
    "\n",
    "---\n",
    "\n",
    "## 48. Inferencia y Predicciones\n",
    "\n",
    "### Context Manager torch.no_grad()\n",
    "\n",
    "Desactiva el cálculo de gradientes para inferencia:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    test_output = and_model(and_input)\n",
    "    for prediction in zip(and_input, test_output):\n",
    "        print(f'Input: {prediction[0]} Prediction: {prediction[1]}')\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- Reduce consumo de memoria\n",
    "- Acelera computación\n",
    "- Útil cuando no se necesita backpropagation\n",
    "\n",
    "**Funcionamiento**: `requires_grad=False` para todos los tensores dentro del contexto.\n",
    "\n",
    "---\n",
    "\n",
    "## 49. Checkpoints y Guardado de Modelos\n",
    "\n",
    "### ModelCheckpoint Callback\n",
    "\n",
    "Guarda el modelo periódicamente durante el entrenamiento:\n",
    "\n",
    "```python\n",
    "checkpoint_callback = ModelCheckpoint()\n",
    "\n",
    "# Obtener ruta del mejor modelo\n",
    "print(checkpoint_callback.best_model_path)\n",
    "```\n",
    "\n",
    "**Características**:\n",
    "- Guarda automáticamente en `lightning_logs/`\n",
    "- Versiona los experimentos\n",
    "- Permite recuperar el mejor modelo según métricas\n",
    "\n",
    "### Cargar Modelo desde Checkpoint\n",
    "\n",
    "```python\n",
    "# Cargar modelo entrenado\n",
    "trained_model = ANDModel.load_from_checkpoint(\n",
    "    checkpoint_callback.best_model_path,\n",
    "    model=and_layers\n",
    ")\n",
    "```\n",
    "\n",
    "**Uso**: Recuperar modelos entrenados para inferencia o continuar entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## 50. Predicciones con DataLoader\n",
    "\n",
    "### Inferencia en Batches\n",
    "\n",
    "```python\n",
    "# Crear DataLoader de prueba\n",
    "test_data = torch.utils.data.DataLoader(and_input, batch_size=1)\n",
    "\n",
    "# Hacer predicciones\n",
    "with torch.no_grad():\n",
    "    for tensor in test_data:\n",
    "        prediction = trained_model(tensor)\n",
    "        print(f'Input: {tensor} Prediction: {prediction}')\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- Procesamiento eficiente de grandes conjuntos de datos\n",
    "- Consistencia con pipeline de entrenamiento\n",
    "\n",
    "---\n",
    "\n",
    "## 51. Monitoreo y Visualización\n",
    "\n",
    "### Registro de Métricas con self.log()\n",
    "\n",
    "```python\n",
    "def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.forward(x)\n",
    "    loss = nn.functional.mse_loss(y_hat, y)\n",
    "    \n",
    "    # Registrar métrica\n",
    "    self.log(\"train_loss\", loss)\n",
    "    return loss\n",
    "```\n",
    "\n",
    "**Funcionamiento**: Automáticamente registra métricas para TensorBoard y otros loggers.\n",
    "\n",
    "### Historial Personalizado\n",
    "\n",
    "```python\n",
    "# Almacenar datos de entrenamiento\n",
    "self.history['epochs'].append(self.current_epoch)\n",
    "self.history['loss'].append(loss.detach().numpy())\n",
    "```\n",
    "\n",
    "### Visualización de Pérdida\n",
    "\n",
    "```python\n",
    "# Extraer datos del historial\n",
    "epochs = and_model.history['epochs']\n",
    "loss = and_model.history['loss']\n",
    "\n",
    "# Crear gráfica\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.plot(epochs, loss, label=\"Training loss\")\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.legend(loc='upper right')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 52. Función de Pérdida MSE\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "Función de pérdida comúnmente usada para regresión:\n",
    "\n",
    "```python\n",
    "loss = nn.functional.mse_loss(y_hat, y)\n",
    "```\n",
    "\n",
    "**Fórmula**:\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Características**:\n",
    "- Penaliza errores grandes más que pequeños\n",
    "- Siempre positiva\n",
    "- Diferenciable (útil para backpropagation)\n",
    "\n",
    "---\n",
    "\n",
    "## 53. Estructura de un Proyecto con Lightning\n",
    "\n",
    "### Ejemplo Completo: Operador AND\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# 1. Definir datos\n",
    "and_input = torch.Tensor([[0.,0.], [0.,1.], [1.,0.], [1., 1.]])\n",
    "and_labels = torch.Tensor([[0.],[0.], [0.], [1.]])\n",
    "and_data = TensorDataset(and_input, and_labels)\n",
    "train_loader = DataLoader(and_data, batch_size=4, shuffle=True)\n",
    "\n",
    "# 2. Definir modelo\n",
    "class ANDModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.history = {'epochs': [], 'loss': []}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-2)\n",
    "\n",
    "# 3. Crear arquitectura\n",
    "and_layers = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# 4. Inicializar y entrenar\n",
    "and_model = ANDModel(and_layers)\n",
    "checkpoint_callback = ModelCheckpoint()\n",
    "trainer = L.Trainer(max_epochs=1000, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model=and_model, train_dataloaders=train_loader)\n",
    "\n",
    "# 5. Evaluar\n",
    "with torch.no_grad():\n",
    "    predictions = and_model(and_input)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 54. Conceptos Clave de PyTorch Lightning\n",
    "\n",
    "### Principios Fundamentales\n",
    "\n",
    "1. **Separación de conceptos**: Investigación (modelo) vs. ingeniería (entrenamiento)\n",
    "2. **Métodos del ciclo de vida**: Estructura predecible y mantenible\n",
    "3. **Abstracción del entrenamiento**: Trainer maneja detalles de bajo nivel\n",
    "4. **Callbacks**: Extensibilidad sin modificar código del modelo\n",
    "\n",
    "### Diferencias PyTorch vs PyTorch Lightning\n",
    "\n",
    "| Aspecto | PyTorch | PyTorch Lightning |\n",
    "|---------|---------|-------------------|\n",
    "| **Loops de entrenamiento** | Manual | Automático (Trainer) |\n",
    "| **Estructura** | Flexible, menos guiada | Organizada con métodos del ciclo de vida |\n",
    "| **Backpropagation** | Manual (.backward()) | Automática |\n",
    "| **Logging** | Manual | Integrado (self.log()) |\n",
    "| **Checkpoints** | Manual | Automático (callbacks) |\n",
    "| **Multi-GPU** | Requiere configuración | Automático con parámetro |\n",
    "\n",
    "### Ventajas para Investigación\n",
    "\n",
    "1. **Reproducibilidad**: Código estructurado facilita reproducir experimentos\n",
    "2. **Escalabilidad**: Fácil migración de CPU a GPU/TPU\n",
    "3. **Experimentación rápida**: Menos código boilerplate permite iterar más rápido\n",
    "4. **Colaboración**: Estructura estándar facilita compartir código\n",
    "\n",
    "---\n",
    "\n",
    "## 55. Consejos Prácticos con PyTorch Lightning\n",
    "\n",
    "### Mejores Prácticas\n",
    "\n",
    "1. **Organizar hiperparámetros**: Usar `self.save_hyperparameters()` en `__init__()`\n",
    "   ```python\n",
    "   def __init__(self, learning_rate=1e-3):\n",
    "       super().__init__()\n",
    "       self.save_hyperparameters()\n",
    "   ```\n",
    "\n",
    "2. **Validación**: Implementar `validation_step()` para monitorear overfitting\n",
    "   ```python\n",
    "   def validation_step(self, batch, batch_idx):\n",
    "       x, y = batch\n",
    "       y_hat = self.forward(x)\n",
    "       loss = nn.functional.mse_loss(y_hat, y)\n",
    "       self.log(\"val_loss\", loss)\n",
    "       return loss\n",
    "   ```\n",
    "\n",
    "3. **Early stopping**: Usar callback para detener entrenamiento automáticamente\n",
    "   ```python\n",
    "   from lightning.pytorch.callbacks import EarlyStopping\n",
    "   early_stop = EarlyStopping(monitor=\"val_loss\", patience=3)\n",
    "   ```\n",
    "\n",
    "4. **Logging estructurado**: Usar TensorBoard o Weights & Biases\n",
    "   ```python\n",
    "   from lightning.pytorch.loggers import TensorBoardLogger\n",
    "   logger = TensorBoardLogger(\"logs\", name=\"my_model\")\n",
    "   trainer = L.Trainer(logger=logger)\n",
    "   ```\n",
    "\n",
    "### Aplicaciones Prácticas\n",
    "\n",
    "- **Clasificación de imágenes**: CNNs para visión por computadora\n",
    "- **Procesamiento de lenguaje natural**: Transformers y RNNs\n",
    "- **Redes Generativas**: GANs y VAEs\n",
    "- **Reinforcement Learning**: Algoritmos de aprendizaje por refuerzo\n",
    "- **Investigación**: Prototipado rápido de arquitecturas nuevas\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Clasificación de Dígitos con PyTorch Lightning\n",
    "\n",
    "### 1. Clasificación Multi-clase\n",
    "\n",
    "La clasificación de dígitos es un problema de clasificación multi-clase donde se busca identificar imágenes en escala de grises de dígitos manuscritos (28 × 28 píxeles) en 10 categorías (0 a 9) utilizando el conjunto de datos MNIST.\n",
    "\n",
    "### 2. Importaciones Básicas\n",
    "\n",
    "Es fundamental importar las bibliotecas necesarias para el manejo de datos, visualización y construcción de modelos. Las bibliotecas clave incluyen:\n",
    "- `torch`: Para la construcción de modelos y operaciones tensoriales.\n",
    "- `torchvision`: Para cargar y transformar conjuntos de datos como MNIST y CIFAR10.\n",
    "- `lightning`: Para facilitar la estructura y entrenamiento de modelos en PyTorch.\n",
    "\n",
    "### 3. Carga y Visualización de Datos\n",
    "\n",
    "La carga de datos se realiza utilizando `torchvision.datasets`, y es importante visualizar los datos para entender su distribución y características. Esto se puede hacer utilizando `matplotlib` para mostrar imágenes y sus etiquetas.\n",
    "\n",
    "### 4. Preparación de Datos\n",
    "\n",
    "Se utilizan `DataLoader` y `TensorDataset` para manejar los datos de manera eficiente. Esto permite dividir los datos en lotes y mezclar los datos durante el entrenamiento.\n",
    "\n",
    "### 5. Definición del Modelo\n",
    "\n",
    "Se define un modelo utilizando `LightningModule`, que organiza el código y separa la lógica del modelo de los detalles de entrenamiento. Los métodos esenciales incluyen:\n",
    "- `__init__()`: Inicializa el modelo y define las capas.\n",
    "- `forward()`: Define cómo los datos pasan a través del modelo.\n",
    "- `training_step()`: Define un paso de entrenamiento y calcula la pérdida.\n",
    "- `configure_optimizers()`: Configura el optimizador para el modelo.\n",
    "\n",
    "### 6. Entrenamiento del Modelo\n",
    "\n",
    "El entrenamiento se realiza utilizando la clase `Trainer` de PyTorch Lightning, que maneja automáticamente los bucles de entrenamiento, la retropropagación y el registro de métricas. Se pueden utilizar callbacks como `ModelCheckpoint` para guardar el modelo durante el entrenamiento.\n",
    "\n",
    "### 7. Evaluación y Predicciones\n",
    "\n",
    "Después de entrenar el modelo, se evalúa su rendimiento utilizando un conjunto de datos de prueba. Se pueden realizar predicciones y visualizar los resultados para entender cómo el modelo clasifica las imágenes.\n",
    "\n",
    "### 8. Redes Neuronales Convolucionales (CNN)\n",
    "\n",
    "Se introduce el concepto de redes neuronales convolucionales, que son más efectivas para el procesamiento de imágenes. Las CNN aprenden patrones locales en las imágenes y son capaces de reconocer características complejas a través de múltiples capas.\n",
    "\n",
    "### 9. Implementación de CNN\n",
    "\n",
    "Se define una arquitectura de CNN utilizando `torch.nn.Sequential`, que incluye capas convolucionales, capas de normalización y capas completamente conectadas. Se entrena el modelo de la misma manera que se haría con un modelo denso, pero aprovechando las ventajas de las CNN.\n",
    "\n",
    "### 10. Visualización de Resultados\n",
    "\n",
    "Es importante visualizar las métricas de entrenamiento y validación, así como las predicciones del modelo en un conjunto de datos de prueba. Esto ayuda a entender el rendimiento del modelo y a identificar áreas de mejora.\n",
    "\n",
    "### 11. Desafío\n",
    "\n",
    "Se propone un desafío para aplicar los conceptos aprendidos en el conjunto de datos CIFAR10, que es más complejo y requiere ajustes en la arquitectura del modelo y en el proceso de entrenamiento.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Entrenamiento de una ConvNet desde Cero en un Conjunto de Datos Pequeño\n",
    "\n",
    "### 1. Introducción a la Clasificación de Imágenes\n",
    "\n",
    "La clasificación de imágenes es una tarea fundamental en visión por computadora, donde el objetivo es asignar una etiqueta a una imagen basada en su contenido. Este proceso se vuelve desafiante cuando se dispone de un conjunto de datos pequeño, lo que es común en aplicaciones del mundo real.\n",
    "\n",
    "### 2. Preparación del Conjunto de Datos\n",
    "\n",
    "#### 2.1. Estructura del Conjunto de Datos\n",
    "El conjunto de datos utilizado en este ejemplo contiene imágenes de dos clases: gatos y perros. Se organiza en carpetas, donde cada carpeta representa una clase. La correcta organización es crucial para que el modelo pueda aprender de manera efectiva.\n",
    "\n",
    "#### 2.2. División de Datos\n",
    "Los datos se dividen en conjuntos de entrenamiento, validación y prueba. Esta división permite evaluar el rendimiento del modelo en datos no vistos y ajustar hiperparámetros durante el entrenamiento.\n",
    "\n",
    "### 3. Importaciones y Configuración Inicial\n",
    "\n",
    "Se utilizan varias bibliotecas para facilitar el proceso de entrenamiento, incluyendo:\n",
    "- **PyTorch**: Para la construcción y entrenamiento del modelo.\n",
    "- **Torchvision**: Para la manipulación de imágenes y conjuntos de datos.\n",
    "- **Lightning**: Para simplificar el proceso de entrenamiento y gestión de experimentos.\n",
    "\n",
    "### 4. Transformaciones de Imágenes\n",
    "\n",
    "Las transformaciones son esenciales para preparar las imágenes antes de ser alimentadas al modelo. Estas pueden incluir:\n",
    "- **Redimensionamiento**: Ajustar las imágenes a un tamaño uniforme.\n",
    "- **Normalización**: Escalar los valores de píxeles para mejorar la convergencia del modelo.\n",
    "- **Aumento de Datos**: Aplicar transformaciones aleatorias para aumentar la diversidad del conjunto de entrenamiento y reducir el sobreajuste.\n",
    "\n",
    "\n",
    "### 5. Creación del Modelo\n",
    "\n",
    "#### 5.1. Arquitectura de la Red\n",
    "Se define una red neuronal convolucional (CNN) que consiste en varias capas convolucionales seguidas de capas de agrupamiento (pooling). Esta arquitectura permite extraer características jerárquicas de las imágenes.\n",
    "\n",
    "#### 5.2. Definición del Modelo en PyTorch Lightning\n",
    "El modelo se encapsula en una clase que hereda de `LightningModule`, lo que permite organizar el código y separar la lógica del modelo de los detalles del entrenamiento.\n",
    "\n",
    "### 6. Entrenamiento del Modelo\n",
    "\n",
    "#### 6.1. Configuración del Entrenador\n",
    "Se utiliza la clase `Trainer` de PyTorch Lightning para manejar el ciclo de entrenamiento. Esto incluye la gestión de épocas, el registro de métricas y la optimización de parámetros.\n",
    "\n",
    "#### 6.2. Monitoreo de Métricas\n",
    "Durante el entrenamiento, se registran métricas como la pérdida y la precisión para evaluar el rendimiento del modelo. Esto permite realizar ajustes en tiempo real y mejorar la calidad del modelo.\n",
    "\n",
    "### 7. Evaluación del Modelo\n",
    "\n",
    "Después del entrenamiento, se evalúa el modelo utilizando el conjunto de prueba. Se calcula la precisión y se visualizan las predicciones para entender cómo el modelo clasifica las imágenes.\n",
    "\n",
    "### 8. Desafío Práctico\n",
    "\n",
    "Se propone un desafío para aplicar los conceptos aprendidos en un nuevo conjunto de datos, como el de mariposas de Kaggle. Esto permite a los estudiantes practicar y consolidar su comprensión de los temas tratados.\n",
    "\n",
    "### 9. Conclusiones\n",
    "\n",
    "El entrenamiento de una ConvNet desde cero en un conjunto de datos pequeño es un proceso que requiere atención a la preparación de datos, la arquitectura del modelo y el monitoreo de métricas. Con las herramientas adecuadas, es posible construir modelos efectivos incluso con recursos limitados.\n",
    "\n",
    "## Mejora de Modelos de Visión por Computadora\n",
    "\n",
    "### 1. Introducción a la Mejora de Modelos\n",
    "La mejora de modelos en visión por computadora se centra en optimizar el rendimiento de los algoritmos de aprendizaje automático. Esto incluye técnicas para mejorar la precisión, reducir el sobreajuste y aumentar la generalización del modelo.\n",
    "\n",
    "### 2. Preprocesamiento de Imágenes\n",
    "El preprocesamiento es crucial para preparar los datos antes de ser alimentados al modelo. Las técnicas comunes incluyen:\n",
    "- **Redimensionamiento**: Ajustar las imágenes a un tamaño uniforme para asegurar que todas las entradas tengan la misma dimensión.\n",
    "- **Normalización**: Escalar los valores de píxeles para que estén en un rango específico, lo que ayuda a la convergencia del modelo.\n",
    "- **Aumento de Datos**: Aplicar transformaciones aleatorias (como rotaciones, recortes y cambios de brillo) para aumentar la diversidad del conjunto de entrenamiento y reducir el riesgo de sobreajuste.\n",
    "\n",
    "### 3. Arquitectura de Redes Neuronales\n",
    "Las redes neuronales convolucionales (CNN) son fundamentales en la mejora de modelos de visión por computadora. Las CNN están diseñadas para reconocer patrones en imágenes a través de capas convolucionales que extraen características jerárquicas. Los componentes clave incluyen:\n",
    "- **Capas Convolucionales**: Extraen características locales de las imágenes.\n",
    "- **Capas de Agrupamiento (Pooling)**: Reducen la dimensionalidad y ayudan a mantener las características más relevantes.\n",
    "- **Capas Completamente Conectadas**: Realizan la clasificación final basándose en las características extraídas.\n",
    "\n",
    "### 4. Técnicas de Regularización\n",
    "Para evitar el sobreajuste, se pueden aplicar varias técnicas de regularización:\n",
    "- **Dropout**: Desactiva aleatoriamente un porcentaje de neuronas durante el entrenamiento para prevenir que el modelo dependa demasiado de ciertas características.\n",
    "- **Regularización L2**: Añade un término de penalización a la función de pérdida para controlar el tamaño de los pesos del modelo.\n",
    "\n",
    "### 5. Optimización de Hiperparámetros\n",
    "La optimización de hiperparámetros es esencial para mejorar el rendimiento del modelo. Esto incluye ajustar parámetros como la tasa de aprendizaje, el tamaño del lote y el número de épocas. Técnicas como la búsqueda en cuadrícula y la búsqueda aleatoria son útiles para encontrar la mejor combinación de hiperparámetros.\n",
    "\n",
    "### 6. Evaluación del Modelo\n",
    "La evaluación del modelo se realiza utilizando métricas como la precisión, la recuperación y la puntuación F1. Es importante utilizar un conjunto de datos de prueba que no haya sido visto durante el entrenamiento para obtener una evaluación precisa del rendimiento del modelo.\n",
    "\n",
    "### 7. Visualización de Resultados\n",
    "La visualización de los resultados es crucial para entender cómo el modelo está funcionando. Esto puede incluir:\n",
    "- **Matriz de Confusión**: Muestra el rendimiento del modelo en cada clase.\n",
    "- **Curvas de Aprendizaje**: Muestran cómo la pérdida y la precisión cambian a lo largo del tiempo durante el entrenamiento.\n",
    "\n",
    "### 8. Conclusiones\n",
    "La mejora de modelos en visión por computadora es un proceso iterativo que implica la preparación de datos, la selección de la arquitectura adecuada, la optimización de hiperparámetros y la evaluación del rendimiento. Con un enfoque sistemático, es posible construir modelos robustos y precisos.\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "# **Pytorch**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Introducción a PyTorch\n",
    "\n",
    "### 1. ¿Qué es PyTorch?\n",
    "PyTorch es una biblioteca de aprendizaje automático de código abierto que permite a los desarrolladores crear y entrenar modelos de aprendizaje profundo. Es conocida por su flexibilidad y facilidad de uso, lo que la convierte en una opción popular entre investigadores y desarrolladores.\n",
    "\n",
    "### 2. Tensores\n",
    "Los tensores son la estructura de datos fundamental en PyTorch. Son similares a los arrays de NumPy, pero pueden ser utilizados en GPUs para acelerar el cálculo. Los tensores pueden ser de diferentes dimensiones:\n",
    "- **Escalar**: Un solo valor.\n",
    "- **Vector**: Una lista de valores (1D).\n",
    "- **Matriz**: Una tabla de valores (2D).\n",
    "- **Tensor de N dimensiones**: Generalización de matrices a más dimensiones.\n",
    "\n",
    "### 3. Operaciones con Tensores\n",
    "PyTorch permite realizar diversas operaciones con tensores, como:\n",
    "- **Aritmética**: Suma, resta, multiplicación y división.\n",
    "- **Transposición**: Cambiar la forma de un tensor.\n",
    "- **Reducción**: Operaciones como suma o promedio a lo largo de una dimensión.\n",
    "\n",
    "### 4. Autograd\n",
    "El módulo `autograd` de PyTorch permite la diferenciación automática. Esto significa que PyTorch puede calcular automáticamente los gradientes de los tensores, lo cual es esencial para el entrenamiento de modelos de aprendizaje profundo. Al definir un tensor con `requires_grad=True`, PyTorch rastrea todas las operaciones realizadas sobre él.\n",
    "\n",
    "### 5. Construcción de Modelos\n",
    "Los modelos en PyTorch se construyen utilizando la clase `nn.Module`. Esta clase permite definir la arquitectura del modelo, incluyendo las capas y la función de activación. Los pasos básicos son:\n",
    "- **Definir el modelo**: Crear una clase que herede de `nn.Module`.\n",
    "- **Inicializar capas**: Definir las capas en el método `__init__()`.\n",
    "- **Definir el paso hacia adelante**: Implementar el método `forward()` para especificar cómo los datos fluyen a través del modelo.\n",
    "\n",
    "### 6. Entrenamiento de Modelos\n",
    "El proceso de entrenamiento en PyTorch implica:\n",
    "- **Definir la función de pérdida**: Medir qué tan bien está funcionando el modelo.\n",
    "- **Seleccionar un optimizador**: Actualizar los pesos del modelo basándose en los gradientes calculados.\n",
    "- **Iterar sobre los datos**: Pasar los datos a través del modelo, calcular la pérdida, realizar la retropropagación y actualizar los pesos.\n",
    "\n",
    "### 7. Evaluación del Modelo\n",
    "Después del entrenamiento, es crucial evaluar el rendimiento del modelo utilizando un conjunto de datos de prueba. Las métricas comunes incluyen precisión, recuperación y puntuación F1. Esto ayuda a entender cómo se comporta el modelo en datos no vistos.\n",
    "\n",
    "### 8. Visualización\n",
    "La visualización de resultados es fundamental para interpretar el rendimiento del modelo. Herramientas como Matplotlib pueden ser utilizadas para graficar la pérdida y la precisión a lo largo de las épocas, así como para mostrar ejemplos de predicciones.\n",
    "\n",
    "### 9. Conclusiones\n",
    "PyTorch es una herramienta poderosa para el desarrollo de modelos de aprendizaje profundo. Su flexibilidad y facilidad de uso lo hacen ideal tanto para principiantes como para expertos. Comprender los conceptos clave, como tensores, autograd y la construcción de modelos, es esencial para aprovechar al máximo esta biblioteca.\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "\n",
    "## 56. PyTorch Workflow Fundamentals\n",
    "\n",
    "### Definición\n",
    "El flujo de trabajo de PyTorch es un conjunto de pasos estándar para construir, entrenar y evaluar modelos de machine learning y deep learning. Este workflow proporciona una estructura organizada que puede adaptarse según las necesidades del problema específico.\n",
    "\n",
    "**Concepto central**: Machine learning y deep learning consisten en tomar datos del pasado, construir un algoritmo para descubrir patrones y usar esos patrones para predecir el futuro.\n",
    "\n",
    "### Diagrama del Workflow de PyTorch\n",
    "\n",
    "El workflow estándar de PyTorch incluye los siguientes pasos principales:\n",
    "\n",
    "1. **Preparación de datos (Data)**\n",
    "2. **Construcción del modelo (Build model)**\n",
    "3. **Ajuste del modelo (Fitting the model)**\n",
    "4. **Hacer predicciones (Making predictions)**\n",
    "5. **Evaluación del modelo (Evaluating the model)**\n",
    "6. **Mejorar mediante experimentación (Improve through experimentation)**\n",
    "7. **Guardar y recargar el modelo (Save and reload the model)**\n",
    "\n",
    "---\n",
    "\n",
    "## 57. Preparación y Carga de Datos\n",
    "\n",
    "### Concepto de \"Datos\" en Machine Learning\n",
    "\n",
    "Los datos en machine learning pueden ser casi cualquier cosa:\n",
    "- Tablas de números (como hojas de cálculo de Excel)\n",
    "- Imágenes de cualquier tipo\n",
    "- Videos\n",
    "- Archivos de audio (canciones, podcasts)\n",
    "- Estructuras de proteínas\n",
    "- Texto\n",
    "\n",
    "### Machine Learning como un Juego de Dos Partes\n",
    "\n",
    "![Concepto clave](concepto)\n",
    "\n",
    "**Parte 1**: Convertir tus datos (sean lo que sean) en números (una representación).\n",
    "\n",
    "**Parte 2**: Elegir o construir un modelo para aprender la representación lo mejor posible.\n",
    "\n",
    "### Creación de Datos: Regresión Lineal\n",
    "\n",
    "```python\n",
    "# Crear parámetros conocidos\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Crear datos\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step)\n",
    "print(X.shape)  # torch.Size([50])\n",
    "\n",
    "X = X.unsqueeze(dim=1)\n",
    "print(X.shape)  # torch.Size([50, 1])\n",
    "\n",
    "y = weight * X + bias\n",
    "\n",
    "print(f\"X:\\n {X[:10]}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y:\\n {y[:10]}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "```\n",
    "\n",
    "**Conceptos importantes**:\n",
    "- `torch.arange()`: Crea un tensor con valores en un rango específico\n",
    "- `.unsqueeze()`: Añade una dimensión al tensor\n",
    "- Los parámetros `weight` y `bias` son valores conocidos que el modelo intentará aprender\n",
    "\n",
    "### División de Datos: Training y Test Sets\n",
    "\n",
    "Uno de los pasos más importantes en un proyecto de machine learning es crear conjuntos de entrenamiento y prueba.\n",
    "\n",
    "#### Propósito de Cada División\n",
    "\n",
    "| Split | Propósito | Cantidad de datos total | ¿Con qué frecuencia se usa? |\n",
    "| ----- | --------- | ----------------------- | --------------------------- |\n",
    "| **Training set** | El modelo aprende de estos datos (como los materiales de estudio durante el semestre) | ~60-80% | Siempre |\n",
    "| **Validation set** | El modelo se ajusta con estos datos (como el examen de práctica antes del examen final) | ~10-20% | A menudo pero no siempre |\n",
    "| **Testing set** | El modelo se evalúa con estos datos para probar lo que ha aprendido (como el examen final) | ~10-20% | Siempre |\n",
    "\n",
    "#### Implementación de la División\n",
    "\n",
    "```python\n",
    "# Crear división train/test\n",
    "train_split = int(0.8 * len(X))  # 80% para entrenamiento, 20% para pruebas\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)\n",
    "# (40, 40, 10, 10)\n",
    "```\n",
    "\n",
    "**Importante**: En problemas del mundo real, esta división debe hacerse al inicio del proyecto. El conjunto de prueba debe mantenerse separado para evaluar qué tan bien el modelo **generaliza** a ejemplos no vistos.\n",
    "\n",
    "### Visualización de Datos\n",
    "\n",
    "```python\n",
    "def plot_predictions(train_data=X_train,\n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=None):\n",
    "    \"\"\"\n",
    "    Grafica datos de entrenamiento, datos de prueba y compara predicciones.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Graficar datos de entrenamiento en azul\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "    \n",
    "    # Graficar datos de prueba en verde\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=6, label=\"Testing data\")\n",
    "    \n",
    "    if predictions is not None:\n",
    "        # Graficar predicciones en rojo\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=5, label=\"Predictions\")\n",
    "    \n",
    "    # Mostrar leyenda\n",
    "    plt.legend(prop={\"size\": 14})\n",
    "\n",
    "plot_predictions()\n",
    "```\n",
    "\n",
    "**Lema del explorador de datos**: \"visualizar, visualizar, visualizar!\"\n",
    "\n",
    "---\n",
    "\n",
    "## 58. Construcción del Modelo\n",
    "\n",
    "### Estructura Básica de un Modelo de PyTorch\n",
    "\n",
    "```python\n",
    "# Crear clase de modelo de Regresión Lineal\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1,\n",
    "                                                dtype=torch.float),\n",
    "                                   requires_grad=True)\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(1,\n",
    "                                            dtype=torch.float),\n",
    "                                requires_grad=True)\n",
    "    \n",
    "    # Forward define la computación en el modelo\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.weights * x + self.bias\n",
    "```\n",
    "\n",
    "### Módulos Esenciales de PyTorch para Construcción de Modelos\n",
    "\n",
    "| Módulo de PyTorch | ¿Qué hace? |\n",
    "| ----------------- | ---------- |\n",
    "| `torch.nn` | Contiene todos los bloques de construcción para grafos computacionales |\n",
    "| `torch.nn.Parameter` | Almacena tensores que pueden usarse con `nn.Module`. Si `requires_grad=True`, los gradientes se calculan automáticamente (autograd) |\n",
    "| `torch.nn.Module` | Clase base para todos los módulos de redes neuronales. Si construyes una red en PyTorch, tu modelo debe heredar de `nn.Module` |\n",
    "| `torch.optim` | Contiene varios algoritmos de optimización |\n",
    "| `def forward()` | Todos los subclases de `nn.Module` requieren un método `forward()` que define la computación |\n",
    "\n",
    "### Componentes del Modelo Anotados\n",
    "\n",
    "**Estructura**:\n",
    "- `nn.Module`: Bloques de construcción más grandes (capas)\n",
    "- `nn.Parameter`: Parámetros más pequeños como pesos y bias\n",
    "- `forward()`: Define cómo hacer cálculos en las entradas dentro de `nn.Module`\n",
    "- `torch.optim`: Métodos de optimización para mejorar parámetros\n",
    "\n",
    "### Verificación del Contenido del Modelo\n",
    "\n",
    "```python\n",
    "# Establecer semilla manual\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Crear instancia del modelo\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Verificar los nn.Parameter dentro del modelo\n",
    "list(model_0.parameters())\n",
    "# [Parameter containing: tensor([0.3367], requires_grad=True),\n",
    "#  Parameter containing: tensor([0.1288], requires_grad=True)]\n",
    "\n",
    "# Obtener el estado del modelo\n",
    "model_0.state_dict()\n",
    "# OrderedDict([('weights', tensor([0.3367])),\n",
    "#              ('bias', tensor([0.1288]))])\n",
    "```\n",
    "\n",
    "**Nota**: Los valores de `weights` y `bias` son aleatorios porque se inicializaron con `torch.randn()`. El modelo comenzará con valores aleatorios e intentará actualizarlos hacia los valores que mejor ajusten los datos.\n",
    "\n",
    "### Hacer Predicciones con torch.inference_mode()\n",
    "\n",
    "```python\n",
    "# Hacer predicciones con el modelo\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)\n",
    "\n",
    "# Verificar predicciones\n",
    "print(f\"Número de muestras de prueba: {len(X_test)}\")\n",
    "print(f\"Número de predicciones hechas: {len(y_preds)}\")\n",
    "print(f\"Valores predichos:\\n{y_preds}\")\n",
    "```\n",
    "\n",
    "**torch.inference_mode()**:\n",
    "- Context manager usado para hacer predicciones\n",
    "- Desactiva funciones necesarias para entrenamiento pero no para inferencia\n",
    "- Hace los forward-passes más rápidos\n",
    "\n",
    "**Nota**: En código antiguo de PyTorch, también se puede ver `torch.no_grad()`. Aunque similares, `torch.inference_mode()` es más nuevo, potencialmente más rápido y preferido.\n",
    "\n",
    "---\n",
    "\n",
    "## 59. Entrenamiento del Modelo\n",
    "\n",
    "### Función de Pérdida y Optimizador\n",
    "\n",
    "Para que el modelo actualice sus parámetros por sí mismo, necesitamos:\n",
    "\n",
    "#### 1. Función de Pérdida (Loss Function)\n",
    "\n",
    "| Función | ¿Qué hace? | Dónde vive en PyTorch | Valores comunes |\n",
    "| ------- | ---------- | --------------------- | --------------- |\n",
    "| **Loss function** | Mide qué tan incorrectas son las predicciones del modelo comparadas con las etiquetas verdaderas | Muchas funciones integradas en `torch.nn` | MAE para regresión (`torch.nn.L1Loss()`), Binary cross entropy para clasificación binaria (`torch.nn.BCELoss()`) |\n",
    "\n",
    "#### 2. Optimizador (Optimizer)\n",
    "\n",
    "| Función | ¿Qué hace? | Dónde vive en PyTorch | Valores comunes |\n",
    "| ------- | ---------- | --------------------- | --------------- |\n",
    "| **Optimizer** | Le dice al modelo cómo actualizar sus parámetros internos para reducir mejor la pérdida | Varias implementaciones en `torch.optim` | SGD (`torch.optim.SGD()`), Adam (`torch.optim.Adam()`) |\n",
    "\n",
    "#### Implementación\n",
    "\n",
    "```python\n",
    "# Crear función de pérdida\n",
    "loss_fn = nn.L1Loss()  # MAE loss = L1Loss\n",
    "\n",
    "# Crear optimizador\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
    "                            lr=0.01)  # lr = learning rate\n",
    "```\n",
    "\n",
    "**Parámetros del optimizador**:\n",
    "- `params`: Parámetros del modelo a optimizar\n",
    "- `lr` (learning rate): Tasa de aprendizaje\n",
    "  - Más alta: actualizaciones más grandes (pueden ser inestables)\n",
    "  - Más baja: actualizaciones más pequeñas (pueden tardar mucho)\n",
    "  - Valores comunes: 0.01, 0.001, 0.0001\n",
    "\n",
    "### Loop de Entrenamiento en PyTorch\n",
    "\n",
    "#### Pasos del Training Loop\n",
    "\n",
    "| Número | Nombre del paso | ¿Qué hace? | Ejemplo de código |\n",
    "| ------ | --------------- | ---------- | ----------------- |\n",
    "| 1 | Forward pass | El modelo pasa por todos los datos de entrenamiento una vez | `model(x_train)` |\n",
    "| 2 | Calcular la pérdida | Las salidas del modelo se comparan con las etiquetas verdaderas | `loss = loss_fn(y_pred, y_train)` |\n",
    "| 3 | Limpiar gradientes | Los gradientes del optimizador se ponen a cero (se acumulan por defecto) | `optimizer.zero_grad()` |\n",
    "| 4 | Realizar backpropagation | Calcula el gradiente de la pérdida respecto a cada parámetro | `loss.backward()` |\n",
    "| 5 | Actualizar el optimizador (gradient descent) | Actualiza los parámetros para mejorarlos | `optimizer.step()` |\n",
    "\n",
    "#### Visualización del Training Loop\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│ 1. Forward Pass: y_pred = model(X_train)       │\n",
    "│                                                 │\n",
    "│ 2. Calculate Loss: loss = loss_fn(y_pred, y)   │\n",
    "│                                                 │\n",
    "│ 3. Zero Gradients: optimizer.zero_grad()       │\n",
    "│                                                 │\n",
    "│ 4. Backpropagation: loss.backward()            │\n",
    "│                                                 │\n",
    "│ 5. Optimizer Step: optimizer.step()            │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Loop de Testing en PyTorch\n",
    "\n",
    "#### Pasos del Testing Loop\n",
    "\n",
    "| Número | Nombre del paso | ¿Qué hace? | Ejemplo de código |\n",
    "| ------ | --------------- | ---------- | ----------------- |\n",
    "| 1 | Forward pass | El modelo pasa por todos los datos de prueba una vez | `model(x_test)` |\n",
    "| 2 | Calcular la pérdida | Las salidas se comparan con las etiquetas verdaderas | `loss = loss_fn(y_pred, y_test)` |\n",
    "| 3 | Calcular métricas de evaluación (opcional) | Junto con la pérdida, puedes calcular otras métricas como accuracy | Funciones personalizadas |\n",
    "\n",
    "**Diferencia clave**: El testing loop NO contiene backpropagation (`loss.backward()`) ni actualización del optimizador (`optimizer.step()`), porque no se están cambiando parámetros.\n",
    "\n",
    "### Implementación Completa del Training Loop\n",
    "\n",
    "```python\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Establecer número de épocas\n",
    "epochs = 100\n",
    "\n",
    "# Crear listas para rastrear valores de pérdida\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    \n",
    "    # Poner modelo en modo entrenamiento\n",
    "    model_0.train()\n",
    "    \n",
    "    # 1. Forward pass\n",
    "    y_pred = model_0(X_train)\n",
    "    \n",
    "    # 2. Calcular pérdida\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # 3. Limpiar gradientes del optimizador\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation de la pérdida\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Avanzar el optimizador\n",
    "    optimizer.step()\n",
    "    \n",
    "    ### Testing\n",
    "    \n",
    "    # Poner modelo en modo evaluación\n",
    "    model_0.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass en datos de prueba\n",
    "        test_pred = model_0(X_test)\n",
    "        \n",
    "        # 2. Calcular pérdida en datos de prueba\n",
    "        test_loss = loss_fn(test_pred, y_test.type(torch.float))\n",
    "        \n",
    "        # Imprimir lo que está sucediendo\n",
    "        if epoch % 10 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss}\")\n",
    "```\n",
    "\n",
    "### Visualización de Curvas de Pérdida\n",
    "\n",
    "```python\n",
    "# Graficar curvas de pérdida\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    "**Curvas de pérdida**: Muestran la pérdida disminuyendo con el tiempo. La pérdida mide qué tan *incorrecto* está el modelo, así que menor es mejor.\n",
    "\n",
    "### Verificación de Parámetros Aprendidos\n",
    "\n",
    "```python\n",
    "# Encontrar parámetros aprendidos del modelo\n",
    "print(\"El modelo aprendió los siguientes valores para weights y bias:\")\n",
    "print(model_0.state_dict())\n",
    "print(\"\\nY los valores originales para weights y bias son:\")\n",
    "print(f\"weights: {weight}, bias: {bias}\")\n",
    "```\n",
    "\n",
    "**Resultado**: El modelo se acerca mucho a calcular los valores exactos originales. En machine learning y deep learning, a menudo se busca una aproximación cercana, no perfección.\n",
    "\n",
    "---\n",
    "\n",
    "## 60. Hacer Predicciones con un Modelo Entrenado (Inferencia)\n",
    "\n",
    "### Reglas para Hacer Predicciones en PyTorch\n",
    "\n",
    "Tres cosas para recordar al hacer predicciones con un modelo de PyTorch:\n",
    "\n",
    "1. **Poner el modelo en modo evaluación** (`model.eval()`)\n",
    "2. **Hacer predicciones usando el context manager de inference mode** (`with torch.inference_mode(): ...`)\n",
    "3. **Todas las predicciones deben hacerse con objetos en el mismo dispositivo** (datos y modelo en GPU o ambos en CPU)\n",
    "\n",
    "### Implementación\n",
    "\n",
    "```python\n",
    "# 1. Poner modelo en modo evaluación\n",
    "model_0.eval()\n",
    "\n",
    "# 2. Configurar context manager de inference mode\n",
    "with torch.inference_mode():\n",
    "    # 3. Asegurar que los cálculos se hagan con modelo y datos en el mismo dispositivo\n",
    "    y_preds = model_0(X_test)\n",
    "\n",
    "y_preds\n",
    "```\n",
    "\n",
    "**Beneficios de inference_mode()**:\n",
    "- Desactiva cálculos útiles para entrenamiento pero innecesarios para inferencia\n",
    "- Resulta en computación más rápida\n",
    "\n",
    "### Visualización de Predicciones\n",
    "\n",
    "```python\n",
    "plot_predictions(predictions=y_preds)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 61. Guardar y Cargar un Modelo de PyTorch\n",
    "\n",
    "### Métodos Principales para Guardar/Cargar\n",
    "\n",
    "| Método de PyTorch | ¿Qué hace? |\n",
    "| ----------------- | ---------- |\n",
    "| `torch.save` | Guarda un objeto serializado en disco usando la utilidad `pickle` de Python |\n",
    "| `torch.load` | Usa las características de unpickling de `pickle` para deserializar y cargar objetos de archivos |\n",
    "| `torch.nn.Module.load_state_dict` | Carga el diccionario de parámetros de un modelo usando un `state_dict()` guardado |\n",
    "\n",
    "**⚠️ Advertencia de seguridad**: El módulo `pickle` no es seguro. Solo debes cargar modelos de PyTorch de fuentes confiables.\n",
    "\n",
    "### Guardar el state_dict() de un Modelo\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Crear directorio de modelos\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Crear ruta de guardado del modelo\n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Guardar el state dict del modelo\n",
    "print(f\"Guardando modelo en: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(),\n",
    "           f=MODEL_SAVE_PATH)\n",
    "```\n",
    "\n",
    "**Convención**: Los modelos de PyTorch guardados suelen terminar en `.pt` o `.pth`.\n",
    "\n",
    "**Verificar archivo guardado**:\n",
    "```python\n",
    "!ls -l models/01_pytorch_workflow_model_0.pth\n",
    "```\n",
    "\n",
    "### Cargar el state_dict() de un Modelo Guardado\n",
    "\n",
    "```python\n",
    "# Instanciar nueva instancia del modelo\n",
    "loaded_model_0 = LinearRegressionModel()\n",
    "\n",
    "# Cargar state_dict del modelo guardado\n",
    "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "```\n",
    "\n",
    "**Por qué cargar en una nueva instancia**:\n",
    "- Solo guardamos el `state_dict()` (diccionario de parámetros), no el modelo completo\n",
    "- Debemos cargar el `state_dict()` en una nueva instancia del modelo\n",
    "\n",
    "**Ventaja**: Guardar solo el `state_dict()` es más flexible que guardar el modelo completo, evitando problemas cuando se refactoriza el código.\n",
    "\n",
    "### Hacer Predicciones con Modelo Cargado\n",
    "\n",
    "```python\n",
    "# 1. Poner modelo cargado en modo evaluación\n",
    "loaded_model_0.eval()\n",
    "\n",
    "# 2. Usar context manager de inference mode\n",
    "with torch.inference_mode():\n",
    "    loaded_model_preds = loaded_model_0(X_test)\n",
    "\n",
    "# Comparar predicciones\n",
    "y_preds == loaded_model_preds\n",
    "# tensor([[True], [True], ...])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 62. Código Device-Agnostic (Independiente del Dispositivo)\n",
    "\n",
    "### Configuración para GPU/CPU\n",
    "\n",
    "```python\n",
    "# Configurar código device-agnostic\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "```\n",
    "\n",
    "**Resultado esperado**:\n",
    "- Con GPU: `Using device: cuda`\n",
    "- Sin GPU: `Using device: cpu`\n",
    "\n",
    "### Modelo con nn.Linear()\n",
    "\n",
    "#### Diferencia entre nn.Parameter y nn.Linear\n",
    "\n",
    "```python\n",
    "class LinearRegressionModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Usar nn.Linear() para crear parámetros del modelo\n",
    "        self.linear_layer = nn.Linear(in_features=1,\n",
    "                                      out_features=1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# Establecer semilla manual\n",
    "torch.manual_seed(42)\n",
    "model_1 = LinearRegressionModelV2()\n",
    "model_1, model_1.state_dict()\n",
    "```\n",
    "\n",
    "**Ventajas de nn.Linear()**:\n",
    "- Crea automáticamente `weight` y `bias` aleatorios\n",
    "- `in_features`: Número de dimensiones de entrada\n",
    "- `out_features`: Número de dimensiones de salida\n",
    "\n",
    "### Mover Modelo al Dispositivo\n",
    "\n",
    "```python\n",
    "# Verificar dispositivo actual del modelo\n",
    "next(model_1.parameters()).device\n",
    "# device(type='cpu')\n",
    "\n",
    "# Mover modelo a GPU (si está disponible)\n",
    "model_1.to(device)\n",
    "next(model_1.parameters()).device\n",
    "# device(type='cuda', index=0) si GPU disponible\n",
    "```\n",
    "\n",
    "### Training Loop Device-Agnostic\n",
    "\n",
    "```python\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# Crear función de pérdida y optimizador\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.01)\n",
    "\n",
    "# Poner datos en el dispositivo disponible\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train()\n",
    "    \n",
    "    # 1. Forward pass\n",
    "    y_pred = model_1(X_train)\n",
    "    \n",
    "    # 2. Calcular pérdida\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # 3. Limpiar gradientes\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Step del optimizador\n",
    "    optimizer.step()\n",
    "    \n",
    "    ### Testing\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_1(X_test)\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n",
    "```\n",
    "\n",
    "**Nota importante**: Debido a la naturaleza aleatoria del machine learning, los resultados pueden variar ligeramente entre CPU y GPU incluso usando la misma semilla.\n",
    "\n",
    "### Hacer Predicciones con Datos en GPU\n",
    "\n",
    "```python\n",
    "# Poner modelo en modo evaluación\n",
    "model_1.eval()\n",
    "\n",
    "# Hacer predicciones\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_1(X_test)\n",
    "\n",
    "# Graficar predicciones (requiere datos en CPU)\n",
    "plot_predictions(predictions=y_preds.cpu())\n",
    "```\n",
    "\n",
    "**Importante**: Bibliotecas como pandas, matplotlib y NumPy no pueden usar datos en GPU. Usa `.cpu()` para copiar tensores a CPU antes de visualizar.\n",
    "\n",
    "---\n",
    "\n",
    "## 63. Conceptos Clave del Workflow de PyTorch\n",
    "\n",
    "### Resumen del Workflow\n",
    "\n",
    "1. **Datos**: Convertir a tensores y dividir en train/test\n",
    "2. **Modelo**: Construir heredando de `nn.Module`\n",
    "3. **Training**: \n",
    "   - Forward pass\n",
    "   - Calcular pérdida\n",
    "   - Limpiar gradientes\n",
    "   - Backpropagation\n",
    "   - Step del optimizador\n",
    "4. **Evaluación**: Forward pass sin actualizar parámetros\n",
    "5. **Predicciones**: Usar `inference_mode()`\n",
    "6. **Guardar/Cargar**: Usar `state_dict()`\n",
    "\n",
    "### Mejores Prácticas\n",
    "\n",
    "1. **Visualizar datos**: Siempre visualizar antes y después del procesamiento\n",
    "2. **Código device-agnostic**: Escribir código que funcione en CPU o GPU\n",
    "3. **Monitorear métricas**: Rastrear pérdida y otras métricas durante entrenamiento\n",
    "4. **Separar train/test**: Mantener datos de prueba completamente separados\n",
    "5. **Experimentar**: Ajustar hiperparámetros y arquitectura del modelo\n",
    "\n",
    "### Hiperparámetros Comunes\n",
    "\n",
    "- **Learning rate**: 0.01, 0.001, 0.0001\n",
    "- **Batch size**: 32, 64, 128\n",
    "- **Epochs**: 100, 500, 1000\n",
    "- **Optimizer**: SGD, Adam, RMSprop\n",
    "\n",
    "---\n",
    "\n",
    "## 64. Clasificación con PyTorch\n",
    "\n",
    "### Definición de Clasificación\n",
    "La clasificación es un problema de machine learning donde se busca predecir a qué categoría o clase pertenece algo. A diferencia de la regresión (que predice números continuos), la clasificación predice etiquetas discretas.\n",
    "\n",
    "**Tipos de problemas de clasificación**:\n",
    "\n",
    "| Tipo | Descripción | Ejemplo |\n",
    "|------|-------------|---------|\n",
    "| **Clasificación binaria** | El objetivo puede ser una de dos opciones (sí o no) | Predecir si alguien tiene enfermedad cardíaca basándose en parámetros de salud |\n",
    "| **Clasificación multi-clase** | El objetivo puede ser una de más de dos opciones | Decidir si una foto es de comida, una persona o un perro |\n",
    "| **Clasificación multi-etiqueta** | Al objetivo se le pueden asignar múltiples opciones | Predecir qué categorías deberían asignarse a un artículo de Wikipedia (ej. matemáticas, ciencia y filosofía) |\n",
    "\n",
    "---\n",
    "\n",
    "## 65. Arquitectura de una Red Neuronal de Clasificación\n",
    "\n",
    "### Componentes Principales\n",
    "\n",
    "La siguiente tabla muestra los hiperparámetros típicos para redes neuronales de clasificación:\n",
    "\n",
    "| **Hiperparámetro** | **Clasificación Binaria** | **Clasificación Multi-clase** |\n",
    "| --- | --- | --- |\n",
    "| **Forma de capa de entrada** (`in_features`) | Igual al número de características (ej. 5 para edad, sexo, altura, peso, estado de fumador) | Igual que clasificación binaria |\n",
    "| **Capa(s) oculta(s)** | Específico del problema, mínimo = 1, máximo = ilimitado | Igual que clasificación binaria |\n",
    "| **Neuronas por capa oculta** | Específico del problema, generalmente 10 a 512 | Igual que clasificación binaria |\n",
    "| **Forma de capa de salida** (`out_features`) | 1 (una clase u otra) | 1 por clase (ej. 3 para foto de comida, persona o perro) |\n",
    "| **Activación de capa oculta** | Usualmente ReLU pero pueden ser muchas otras | Igual que clasificación binaria |\n",
    "| **Activación de salida** | Sigmoid | Softmax |\n",
    "| **Función de pérdida** | Binary cross entropy (BCELoss) | Cross entropy (CrossEntropyLoss) |\n",
    "| **Optimizador** | SGD, Adam (ver `torch.optim` para más opciones) | Igual que clasificación binaria |\n",
    "\n",
    "**Nota**: Esta lista variará dependiendo del problema específico en el que estés trabajando.\n",
    "\n",
    "---\n",
    "\n",
    "## 66. Preparación de Datos para Clasificación\n",
    "\n",
    "### Creación de Datos de Clasificación Binaria\n",
    "\n",
    "Para este ejemplo, usaremos el método `make_circles()` de Scikit-Learn para generar dos círculos con puntos de diferentes colores:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Crear 1000 muestras\n",
    "n_samples = 1000\n",
    "\n",
    "# Crear círculos\n",
    "X, y = make_circles(n_samples,\n",
    "                    noise=0.03,  # un poco de ruido a los puntos\n",
    "                    random_state=42)  # semilla aleatoria para reproducibilidad\n",
    "\n",
    "# Ver primeros 5 ejemplos\n",
    "print(f\"Primeras 5 características X:\\n{X[:5]}\")\n",
    "print(f\"\\nPrimeras 5 etiquetas y:\\n{y[:5]}\")\n",
    "```\n",
    "\n",
    "**Resultado esperado**:\n",
    "- `X`: Array de forma (1000, 2) - dos características por muestra\n",
    "- `y`: Array de forma (1000,) - una etiqueta por muestra (0 o 1)\n",
    "\n",
    "### Visualización de Datos\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizar con un gráfico\n",
    "plt.scatter(x=X[:, 0], \n",
    "            y=X[:, 1], \n",
    "            c=y, \n",
    "            cmap=plt.cm.RdYlBu)\n",
    "plt.title(\"Datos de círculos\")\n",
    "plt.xlabel(\"Característica 1\")\n",
    "plt.ylabel(\"Característica 2\")\n",
    "```\n",
    "\n",
    "**Interpretación**: \n",
    "- Puntos rojos representan una clase (y=0)\n",
    "- Puntos azules representan otra clase (y=1)\n",
    "- El objetivo es construir un modelo que separe estos puntos\n",
    "\n",
    "### Verificar Formas de Entrada y Salida\n",
    "\n",
    "```python\n",
    "# Verificar las formas de características y etiquetas\n",
    "print(X.shape, y.shape)  # (1000, 2), (1000,)\n",
    "\n",
    "# Ver primer ejemplo de características y etiquetas\n",
    "X_sample = X[0]\n",
    "y_sample = y[0]\n",
    "print(f\"Valores para una muestra de X: {X_sample} y lo mismo para y: {y_sample}\")\n",
    "print(f\"Formas para una muestra de X: {X_sample.shape} y lo mismo para y: {y_sample.shape}\")\n",
    "```\n",
    "\n",
    "**Resultado**: Cada muestra tiene 2 características (vector) y 1 etiqueta (escalar).\n",
    "\n",
    "---\n",
    "\n",
    "## 67. Conversión a Tensores y División de Datos\n",
    "\n",
    "### Convertir a Tensores de PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Convertir a tensores\n",
    "X_tensor = torch.from_numpy(X).type(torch.float)\n",
    "y_tensor = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# Ver las primeras 5 muestras\n",
    "print(X_tensor[:5])\n",
    "print(y_tensor[:5])\n",
    "```\n",
    "\n",
    "**Importante**: Convertir a `torch.float` es crucial para computaciones posteriores.\n",
    "\n",
    "### División en Conjuntos de Entrenamiento y Prueba\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tensor,\n",
    "    y_tensor,\n",
    "    test_size=0.2,  # 20% prueba, 80% entrenamiento\n",
    "    random_state=42  # hacer la división aleatoria reproducible\n",
    ")\n",
    "\n",
    "print(f\"Tamaño train: {len(X_train)}, Tamaño test: {len(X_test)}\")\n",
    "# Tamaño train: 800, Tamaño test: 200\n",
    "```\n",
    "\n",
    "**Proporción estándar**: 80% entrenamiento, 20% prueba es una división común.\n",
    "\n",
    "---\n",
    "\n",
    "## 68. Construcción de un Modelo de Clasificación\n",
    "\n",
    "### Modelo Básico con Capas Lineales\n",
    "\n",
    "```python\n",
    "from torch import nn\n",
    "\n",
    "# Construir modelo\n",
    "class CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Crear 2 capas nn.Linear capaces de manejar formas de entrada y salida\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5)  # 2 características -> 5 ocultas\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1)  # 5 ocultas -> 1 salida\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # El cálculo pasa primero por layer_1 y luego por layer_2\n",
    "        return self.layer_2(self.layer_1(x))\n",
    "\n",
    "# Crear instancia y enviar al dispositivo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_0 = CircleModelV0().to(device)\n",
    "print(model_0)\n",
    "```\n",
    "\n",
    "**Conceptos clave**:\n",
    "- `in_features=2`: Número de características de entrada\n",
    "- `out_features=5`: Unidades ocultas (hiperparámetro ajustable)\n",
    "- `out_features=1`: Una salida para clasificación binaria\n",
    "\n",
    "### Modelo con nn.Sequential\n",
    "\n",
    "```python\n",
    "# Replicar CircleModelV0 con nn.Sequential\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "print(model_0)\n",
    "```\n",
    "\n",
    "**Ventaja**: `nn.Sequential` es más simple para computaciones directas.  \n",
    "**Desventaja**: Siempre se ejecuta secuencialmente, menos flexible para arquitecturas complejas.\n",
    "\n",
    "---\n",
    "\n",
    "## 69. Función de Pérdida y Optimizador para Clasificación\n",
    "\n",
    "### Selección de Función de Pérdida\n",
    "\n",
    "Para clasificación binaria, PyTorch ofrece dos implementaciones de binary cross entropy:\n",
    "\n",
    "| Función | Descripción |\n",
    "|---------|-------------|\n",
    "| `nn.BCELoss()` | Binary Cross Entropy - requiere aplicar sigmoid a las salidas del modelo manualmente |\n",
    "| `nn.BCEWithLogitsLoss()` | Binary Cross Entropy con Sigmoid incorporado - **más numericamente estable** |\n",
    "\n",
    "```python\n",
    "# Crear función de pérdida\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # Sigmoid incorporado\n",
    "\n",
    "# Crear optimizador\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "**Recomendación**: Usar `BCEWithLogitsLoss()` porque es más estable numéricamente.\n",
    "\n",
    "### Función de Precisión (Accuracy)\n",
    "\n",
    "```python\n",
    "# Calcular precisión (métrica de clasificación)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula precisión entre etiquetas verdaderas y predicciones.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Etiquetas verdaderas\n",
    "        y_pred: Predicciones del modelo\n",
    "    \n",
    "    Returns:\n",
    "        Valor de precisión como porcentaje\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "```\n",
    "\n",
    "**Uso**: La precisión mide qué tan \"correcto\" está el modelo (complementa la función de pérdida).\n",
    "\n",
    "---\n",
    "\n",
    "## 70. De Logits a Etiquetas de Predicción\n",
    "\n",
    "### Proceso de Transformación\n",
    "\n",
    "El modelo produce salidas en tres etapas:\n",
    "\n",
    "**Logits** → **Probabilidades de Predicción** → **Etiquetas de Predicción**\n",
    "\n",
    "#### 1. Logits (Salidas Crudas)\n",
    "\n",
    "```python\n",
    "# Ver las primeras 5 salidas del forward pass en datos de prueba\n",
    "y_logits = model_0(X_test.to(device))[:5]\n",
    "print(f\"Logits:\\n{y_logits}\")\n",
    "```\n",
    "\n",
    "**Logits**: Salidas crudas de la ecuación lineal $y = x \\cdot W^T + b$\n",
    "\n",
    "#### 2. Probabilidades de Predicción\n",
    "\n",
    "```python\n",
    "# Aplicar función de activación sigmoid\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "print(f\"Probabilidades:\\n{y_pred_probs}\")\n",
    "```\n",
    "\n",
    "**Función Sigmoid**: \n",
    "$$\\text{sigmoid}(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Convierte logits a valores entre 0 y 1 (probabilidades).\n",
    "\n",
    "#### 3. Etiquetas de Predicción\n",
    "\n",
    "```python\n",
    "# Redondear probabilidades a etiquetas\n",
    "y_preds = torch.round(y_pred_probs)\n",
    "print(f\"Etiquetas:\\n{y_preds}\")\n",
    "\n",
    "# En una sola línea\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
    "```\n",
    "\n",
    "**Regla de decisión**:\n",
    "- Si `y_pred_probs` >= 0.5 → `y=1` (clase 1)\n",
    "- Si `y_pred_probs` < 0.5 → `y=0` (clase 0)\n",
    "\n",
    "### Eliminar Dimensiones Extra\n",
    "\n",
    "```python\n",
    "# Eliminar dimensión extra con squeeze()\n",
    "y_preds.squeeze()\n",
    "```\n",
    "\n",
    "**squeeze()**: Elimina dimensiones de tamaño 1.\n",
    "\n",
    "---\n",
    "\n",
    "## 71. Loop de Entrenamiento para Clasificación\n",
    "\n",
    "### Estructura del Loop\n",
    "\n",
    "```python\n",
    "# Establecer semilla\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configurar épocas\n",
    "epochs = 100\n",
    "\n",
    "# Poner datos en el dispositivo\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop de entrenamiento\n",
    "for epoch in range(epochs):\n",
    "    ### Entrenamiento\n",
    "    model_0.train()\n",
    "    \n",
    "    # 1. Forward pass (modelo produce logits crudos)\n",
    "    y_logits = model_0(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))  # logits -> probabilidades -> etiquetas\n",
    "    \n",
    "    # 2. Calcular pérdida/precisión\n",
    "    loss = loss_fn(y_logits, y_train)  # BCEWithLogitsLoss funciona con logits crudos\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "    \n",
    "    # 3. Limpiar gradientes del optimizador\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Step del optimizador\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Imprimir progreso cada 10 épocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}%\")\n",
    "```\n",
    "\n",
    "### Loop de Testing\n",
    "\n",
    "```python\n",
    "### Testing\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    # 1. Forward pass\n",
    "    test_logits = model_0(X_test).squeeze()\n",
    "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "    \n",
    "    # 2. Calcular pérdida/precisión\n",
    "    test_loss = loss_fn(test_logits, y_test)\n",
    "    test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "```\n",
    "\n",
    "**Observación**: Si la precisión es ~50%, el modelo está adivinando aleatoriamente (problema común).\n",
    "\n",
    "---\n",
    "\n",
    "## 72. Visualización de Límites de Decisión\n",
    "\n",
    "### Función de Visualización\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Grafica límites de decisión del modelo comparando predicciones con valores reales.\n",
    "    \"\"\"\n",
    "    # Poner todo en CPU\n",
    "    model.to(\"cpu\")\n",
    "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "    \n",
    "    # Configurar límites de predicción y grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), \n",
    "                         np.linspace(y_min, y_max, 101))\n",
    "    \n",
    "    # Crear características\n",
    "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_logits = model(X_to_pred_on)\n",
    "    \n",
    "    # Convertir a etiquetas de predicción\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "    \n",
    "    # Reshape y graficar\n",
    "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "```\n",
    "\n",
    "### Visualizar Resultados\n",
    "\n",
    "```python\n",
    "# Graficar límites de decisión para conjuntos de entrenamiento y prueba\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_0, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_0, X_test, y_test)\n",
    "```\n",
    "\n",
    "**Resultado**: Si el modelo solo dibuja líneas rectas para datos circulares, está **underfitting** (subajuste).\n",
    "\n",
    "---\n",
    "\n",
    "## 73. Mejorando el Modelo: No-Linealidad\n",
    "\n",
    "### Problema del Underfitting\n",
    "\n",
    "El modelo está intentando separar datos circulares con líneas rectas, lo cual es matemáticamente imposible.\n",
    "\n",
    "**Solución**: Añadir funciones de activación no lineales.\n",
    "\n",
    "### Técnicas de Mejora\n",
    "\n",
    "| Técnica | ¿Qué hace? |\n",
    "|---------|------------|\n",
    "| **Añadir más capas** | Aumenta potencialmente las capacidades de aprendizaje |\n",
    "| **Añadir más unidades ocultas** | Hace la red más \"ancha\" |\n",
    "| **Entrenar por más tiempo** | Más oportunidades de aprender patrones |\n",
    "| **Cambiar funciones de activación** | Permite aprender patrones no lineales |\n",
    "| **Cambiar learning rate** | Ajusta la magnitud de actualización de parámetros |\n",
    "| **Cambiar función de pérdida** | Debe coincidir con el tipo de problema |\n",
    "\n",
    "### Modelo con No-Linealidad (ReLU)\n",
    "\n",
    "```python\n",
    "class CircleModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "        self.relu = nn.ReLU()  # Función de activación ReLU\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Interponer ReLU entre capas\n",
    "        z = self.layer_1(x)\n",
    "        z = self.relu(z)\n",
    "        z = self.layer_2(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.layer_3(z)\n",
    "        return z\n",
    "\n",
    "model_3 = CircleModelV2().to(device)\n",
    "print(model_3)\n",
    "```\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**:\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "Convierte todos los valores negativos a 0, manteniendo los positivos.\n",
    "\n",
    "### Entrenar Modelo con No-Linealidad\n",
    "\n",
    "```python\n",
    "# Configurar pérdida y optimizador\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)\n",
    "\n",
    "# Entrenar por más épocas\n",
    "torch.manual_seed(42)\n",
    "epochs = 2500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Entrenamiento\n",
    "    model_3.train()\n",
    "    \n",
    "    y_logits = model_3(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "    \n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Imprimir progreso\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}%\")\n",
    "\n",
    "# Testing\n",
    "model_3.eval()\n",
    "with torch.inference_mode():\n",
    "    test_logits = model_3(X_test).squeeze()\n",
    "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "    test_loss = loss_fn(test_logits, y_test)\n",
    "    test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "    print(f\"Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "```\n",
    "\n",
    "**Resultado esperado**: Precisión significativamente mayor (>90%) gracias a las funciones no lineales.\n",
    "\n",
    "---\n",
    "\n",
    "## 74. Replicando Funciones de Activación No Lineales\n",
    "\n",
    "### Función ReLU Personalizada\n",
    "\n",
    "```python\n",
    "# Crear tensor de prueba\n",
    "A = torch.arange(-10, 10, 1, dtype=torch.float32)\n",
    "\n",
    "# Crear función ReLU manualmente\n",
    "def relu(x):\n",
    "    return torch.maximum(torch.tensor(0), x)\n",
    "\n",
    "# Aplicar ReLU\n",
    "print(f\"Entrada:\\n{A}\")\n",
    "print(f\"Salida ReLU:\\n{relu(A)}\")\n",
    "\n",
    "# Visualizar\n",
    "plt.plot(A, label=\"Input\")\n",
    "plt.plot(relu(A), label=\"ReLU\")\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    "**Efecto**: Todos los valores negativos se convierten en 0.\n",
    "\n",
    "### Función Sigmoid Personalizada\n",
    "\n",
    "```python\n",
    "# Crear función sigmoid personalizada\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "# Aplicar sigmoid\n",
    "print(f\"Salida Sigmoid:\\n{sigmoid(A)}\")\n",
    "\n",
    "# Visualizar\n",
    "plt.plot(sigmoid(A))\n",
    "plt.title(\"Función Sigmoid\")\n",
    "```\n",
    "\n",
    "**Fórmula**:\n",
    "$$S(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "**Efecto**: Comprime valores a rango (0, 1).\n",
    "\n",
    "### Comparación Visual\n",
    "\n",
    "```python\n",
    "# Comparar todas las funciones\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(A)\n",
    "plt.title(\"Lineal (Identidad)\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(relu(A))\n",
    "plt.title(\"ReLU\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(sigmoid(A))\n",
    "plt.title(\"Sigmoid\")\n",
    "```\n",
    "\n",
    "**Concepto clave**: Combinando líneas lineales (rectas) y no lineales (curvas), las redes neuronales pueden aproximar casi cualquier función.\n",
    "\n",
    "---\n",
    "\n",
    "## 75. Clasificación Multi-clase\n",
    "\n",
    "### Diferencia con Clasificación Binaria\n",
    "\n",
    "| Aspecto | Binaria | Multi-clase |\n",
    "|---------|---------|-------------|\n",
    "| **Clases** | 2 (perro vs gato) | >2 (perro vs gato vs pollo) |\n",
    "| **Salidas del modelo** | 1 (probabilidad de clase 1) | 1 por clase |\n",
    "| **Activación de salida** | Sigmoid | Softmax |\n",
    "| **Función de pérdida** | BCEWithLogitsLoss | CrossEntropyLoss |\n",
    "\n",
    "### Creación de Datos Multi-clase\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Configurar hiperparámetros\n",
    "NUM_CLASSES = 4\n",
    "NUM_FEATURES = 2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Crear datos multi-clase\n",
    "X_blob, y_blob = make_blobs(\n",
    "    n_samples=1000,\n",
    "    n_features=NUM_FEATURES,\n",
    "    centers=NUM_CLASSES,\n",
    "    cluster_std=1.5,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Convertir a tensores\n",
    "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
    "\n",
    "# Dividir datos\n",
    "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(\n",
    "    X_blob,\n",
    "    y_blob,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu)\n",
    "plt.title(\"Datos Multi-clase (4 clases)\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 76. Modelo de Clasificación Multi-clase\n",
    "\n",
    "### Arquitectura\n",
    "\n",
    "```python\n",
    "class BlobModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        \"\"\"\n",
    "        Inicializa modelo de clasificación multi-clase.\n",
    "        \n",
    "        Args:\n",
    "            input_features: Número de características de entrada\n",
    "            output_features: Número de clases de salida\n",
    "            hidden_units: Número de unidades ocultas entre capas\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Crear instancia\n",
    "model_4 = BlobModel(\n",
    "    input_features=NUM_FEATURES,\n",
    "    output_features=NUM_CLASSES,\n",
    "    hidden_units=8\n",
    ").to(device)\n",
    "\n",
    "print(model_4)\n",
    "```\n",
    "\n",
    "**Puntos clave**:\n",
    "- `output_features=NUM_CLASSES`: Una salida por clase\n",
    "- Sin activación en la última capa (CrossEntropyLoss la incluye)\n",
    "\n",
    "### Función de Pérdida y Optimizador\n",
    "\n",
    "```python\n",
    "# Crear función de pérdida\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Crear optimizador\n",
    "optimizer = torch.optim.SGD(model_4.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "**CrossEntropyLoss**: Combina softmax y negative log-likelihood, ideal para multi-clase.\n",
    "\n",
    "---\n",
    "\n",
    "## 77. Logits a Etiquetas en Multi-clase\n",
    "\n",
    "### Proceso de Transformación\n",
    "\n",
    "**Logits** → **Softmax (Probabilidades)** → **Argmax (Etiquetas)**\n",
    "\n",
    "#### 1. Logits del Modelo\n",
    "\n",
    "```python\n",
    "# Hacer predicciones\n",
    "y_logits = model_4(X_blob_test.to(device))\n",
    "print(f\"Logits:\\n{y_logits[:5]}\")\n",
    "```\n",
    "\n",
    "#### 2. Aplicar Softmax\n",
    "\n",
    "```python\n",
    "# Convertir logits a probabilidades de predicción\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "print(f\"Probabilidades:\\n{y_pred_probs[:5]}\")\n",
    "\n",
    "# Verificar que suman 1\n",
    "print(f\"Suma de primera muestra: {torch.sum(y_pred_probs[0])}\")\n",
    "```\n",
    "\n",
    "**Función Softmax**:\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
    "\n",
    "Convierte logits a distribución de probabilidad (suma = 1).\n",
    "\n",
    "**Parámetro dim=1**: Aplicar softmax a lo largo de las columnas (por fila).\n",
    "\n",
    "#### 3. Obtener Etiquetas\n",
    "\n",
    "```python\n",
    "# Obtener índice del valor máximo (clase más probable)\n",
    "print(f\"Probabilidades de primera muestra:\\n{y_pred_probs[0]}\")\n",
    "print(f\"Clase predicha: {torch.argmax(y_pred_probs[0])}\")\n",
    "\n",
    "# Para todas las muestras\n",
    "y_preds = torch.argmax(y_pred_probs, dim=1)\n",
    "print(f\"Predicciones: {y_preds[:5]}\\nEtiquetas reales: {y_blob_test[:5]}\")\n",
    "```\n",
    "\n",
    "**torch.argmax()**: Devuelve el índice del valor máximo.\n",
    "\n",
    "---\n",
    "\n",
    "## 78. Entrenamiento de Modelo Multi-clase\n",
    "\n",
    "### Loop de Entrenamiento\n",
    "\n",
    "```python\n",
    "# Establecer semilla\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configurar épocas\n",
    "epochs = 100\n",
    "\n",
    "# Poner datos en dispositivo\n",
    "X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n",
    "X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n",
    "\n",
    "# Loop de entrenamiento y testing\n",
    "for epoch in range(epochs):\n",
    "    ### Entrenamiento\n",
    "    model_4.train()\n",
    "    \n",
    "    # 1. Forward pass (modelo produce logits crudos)\n",
    "    y_logits = model_4(X_blob_train)\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # logits -> probabilidades -> etiquetas\n",
    "    \n",
    "    # 2. Calcular pérdida y precisión\n",
    "    loss = loss_fn(y_logits, y_blob_train)\n",
    "    acc = accuracy_fn(y_true=y_blob_train, y_pred=y_pred)\n",
    "    \n",
    "    # 3. Limpiar gradientes\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Step del optimizador\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Imprimir progreso\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}%\")\n",
    "\n",
    "### Testing\n",
    "model_4.eval()\n",
    "with torch.inference_mode():\n",
    "    test_logits = model_4(X_blob_test)\n",
    "    test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "    test_loss = loss_fn(test_logits, y_blob_test)\n",
    "    test_acc = accuracy_fn(y_true=y_blob_test, y_pred=test_pred)\n",
    "    print(f\"Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")\n",
    "```\n",
    "\n",
    "**Diferencia clave**: Usar `torch.softmax()` y `argmax(dim=1)` para multi-clase.\n",
    "\n",
    "---\n",
    "\n",
    "## 79. Hacer y Evaluar Predicciones Multi-clase\n",
    "\n",
    "### Hacer Predicciones\n",
    "\n",
    "```python\n",
    "# Hacer predicciones\n",
    "model_4.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_4(X_blob_test)\n",
    "\n",
    "# Ver primeras 10 predicciones\n",
    "print(f\"Logits: {y_logits[:10]}\")\n",
    "\n",
    "# Convertir a probabilidades\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "\n",
    "# Convertir a etiquetas\n",
    "y_preds = y_pred_probs.argmax(dim=1)\n",
    "\n",
    "# Comparar con etiquetas reales\n",
    "print(f\"Predicciones: {y_preds[:10]}\\nEtiquetas: {y_blob_test[:10]}\")\n",
    "print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")\n",
    "```\n",
    "\n",
    "### Atajo: Logits a Etiquetas Directamente\n",
    "\n",
    "```python\n",
    "# Saltar softmax y ir directo a etiquetas\n",
    "y_preds = torch.argmax(y_logits, dim=1)\n",
    "```\n",
    "\n",
    "**Nota**: Esto funciona pero pierdes acceso a las probabilidades de predicción.\n",
    "\n",
    "### Visualizar Predicciones\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_4, X_blob_train, y_blob_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_4, X_blob_test, y_blob_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 80. Métricas de Evaluación Adicionales\n",
    "\n",
    "### Métricas Comunes de Clasificación\n",
    "\n",
    "| Métrica | Definición | Código |\n",
    "|---------|------------|--------|\n",
    "| **Accuracy** | De 100 predicciones, ¿cuántas son correctas? | `torchmetrics.Accuracy()` o `sklearn.metrics.accuracy_score()` |\n",
    "| **Precision** | Proporción de verdaderos positivos sobre total de predicciones positivas | `torchmetrics.Precision()` o `sklearn.metrics.precision_score()` |\n",
    "| **Recall** | Proporción de verdaderos positivos sobre total de positivos reales | `torchmetrics.Recall()` o `sklearn.metrics.recall_score()` |\n",
    "| **F1-score** | Combina precision y recall (1 es mejor, 0 es peor) | `torchmetrics.F1Score()` o `sklearn.metrics.f1_score()` |\n",
    "| **Confusion matrix** | Compara predicciones con valores reales en formato tabular | `torchmetrics.ConfusionMatrix()` o `sklearn.metrics.confusion_matrix()` |\n",
    "| **Classification report** | Colección de métricas principales | `sklearn.metrics.classification_report()` |\n",
    "\n",
    "### Usar TorchMetrics\n",
    "\n",
    "```python\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Configurar métrica (asegurar que esté en el dispositivo correcto)\n",
    "torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)\n",
    "\n",
    "# Calcular precisión\n",
    "accuracy = torchmetrics_accuracy(y_preds, y_blob_test)\n",
    "print(f\"Accuracy usando TorchMetrics: {accuracy}\")\n",
    "```\n",
    "\n",
    "**Ventajas de TorchMetrics**:\n",
    "- Diseñado específicamente para PyTorch\n",
    "- Funciona en GPU/CPU\n",
    "- Interfaz consistente\n",
    "\n",
    "---\n",
    "\n",
    "## 81. Conceptos Clave de Clasificación en PyTorch\n",
    "\n",
    "### Resumen del Workflow de Clasificación\n",
    "\n",
    "1. **Preparar datos**:\n",
    "   - Convertir a tensores\n",
    "   - Dividir en train/test\n",
    "   - Visualizar\n",
    "\n",
    "2. **Construir modelo**:\n",
    "   - Clasificación binaria: `output_features=1`, activación Sigmoid\n",
    "   - Multi-clase: `output_features=num_classes`, activación Softmax\n",
    "   - Añadir capas no lineales (ReLU) según necesidad\n",
    "\n",
    "3. **Configurar pérdida y optimizador**:\n",
    "   - Binaria: `BCEWithLogitsLoss`\n",
    "   - Multi-clase: `CrossEntropyLoss`\n",
    "   - Optimizador: SGD o Adam\n",
    "\n",
    "4. **Entrenar**:\n",
    "   - Forward pass → calcular pérdida → backpropagation → actualizar pesos\n",
    "   - Monitorear pérdida y precisión\n",
    "\n",
    "5. **Evaluar**:\n",
    "   - Calcular métricas en conjunto de prueba\n",
    "   - Visualizar límites de decisión\n",
    "\n",
    "6. **Mejorar**:\n",
    "   - Añadir capas/unidades\n",
    "   - Cambiar activaciones\n",
    "   - Ajustar learning rate\n",
    "   - Entrenar más tiempo\n",
    "\n",
    "### Diferencias Clave: Binaria vs Multi-clase\n",
    "\n",
    "| Aspecto | Binaria | Multi-clase |\n",
    "|---------|---------|-------------|\n",
    "| **Salidas** | 1 | `num_classes` |\n",
    "| **Activación** | Sigmoid | Softmax |\n",
    "| **Pérdida** | BCEWithLogitsLoss | CrossEntropyLoss |\n",
    "| **Predicción** | `torch.round(torch.sigmoid(logits))` | `torch.softmax(logits, dim=1).argmax(dim=1)` |\n",
    "\n",
    "### Mejores Prácticas\n",
    "\n",
    "1. **Visualizar datos**: Entender patrones antes de modelar\n",
    "2. **Empezar simple**: Modelo básico primero, luego añadir complejidad\n",
    "3. **Monitorear métricas**: Pérdida y precisión en train/test\n",
    "4. **Usar no-linealidad**: ReLU para patrones complejos\n",
    "5. **Experimentar**: Probar diferentes arquitecturas e hiperparámetros\n",
    "6. **Validar resultados**: Usar métricas múltiples, no solo precisión\n",
    "\n",
    "### Funciones de Activación Comunes\n",
    "\n",
    "| Función | Fórmula | Uso |\n",
    "|---------|---------|-----|\n",
    "| **ReLU** | $\\max(0, x)$ | Capas ocultas |\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-x}}$ | Salida binaria |\n",
    "| **Softmax** | $\\frac{e^{x_i}}{\\sum e^{x_j}}$ | Salida multi-clase |\n",
    "| **Tanh** | $\\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | Capas ocultas (alternativa a ReLU) |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 82. Introducción a Visión por Computadora con PyTorch\n",
    "\n",
    "### Definición\n",
    "La visión por computadora es el arte de enseñar a una computadora a \"ver\". Esto implica construir modelos para clasificar imágenes, detectar objetos, segmentar regiones y más, todo mediante el procesamiento de datos visuales.\n",
    "\n",
    "**Aplicaciones comunes**:\n",
    "- **Clasificación binaria**: ¿Es un gato o un perro?\n",
    "- **Clasificación multi-clase**: ¿Es un gato, perro o pollo?\n",
    "- **Detección de objetos**: ¿Dónde aparece un auto en un video?\n",
    "- **Segmentación**: Separar diferentes objetos en una imagen\n",
    "\n",
    "---\n",
    "\n",
    "## 83. Bibliotecas de Visión por Computadora en PyTorch\n",
    "\n",
    "### Módulos Principales\n",
    "\n",
    "| Módulo PyTorch | ¿Qué hace? |\n",
    "|----------------|------------|\n",
    "| **`torchvision`** | Contiene datasets, arquitecturas de modelos y transformaciones para visión por computadora |\n",
    "| **`torchvision.datasets`** | Datasets de ejemplo (FashionMNIST, CIFAR10, ImageNet) y clases base para datasets personalizados |\n",
    "| **`torchvision.models`** | Arquitecturas pre-entrenadas de modelos (ResNet, VGG, EfficientNet) |\n",
    "| **`torchvision.transforms`** | Transformaciones comunes de imágenes (redimensionar, normalizar, augmentar) |\n",
    "| **`torch.utils.data.Dataset`** | Clase base para crear datasets en PyTorch |\n",
    "| **`torch.utils.data.DataLoader`** | Crea un iterable sobre el dataset para entrenamiento eficiente |\n",
    "\n",
    "### Importaciones Básicas\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verificar versiones\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")\n",
    "```\n",
    "\n",
    "**Nota**: PyTorch >= 1.10.0 y torchvision >= 0.11 son recomendados.\n",
    "\n",
    "---\n",
    "\n",
    "## 84. Dataset FashionMNIST\n",
    "\n",
    "### Descripción\n",
    "FashionMNIST es un dataset creado por Zalando Research que contiene imágenes en escala de grises de 10 tipos diferentes de ropa. Es similar al MNIST clásico (dígitos manuscritos) pero más desafiante.\n",
    "\n",
    "**Características**:\n",
    "- **Tamaño de imagen**: 28×28 píxeles\n",
    "- **Canales de color**: 1 (escala de grises)\n",
    "- **Número de clases**: 10\n",
    "- **Total de imágenes**: 70,000 (60,000 entrenamiento, 10,000 prueba)\n",
    "\n",
    "### Clases del Dataset\n",
    "\n",
    "| Etiqueta | Clase |\n",
    "|----------|-------|\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "### Descargar el Dataset\n",
    "\n",
    "```python\n",
    "# Descargar datos de entrenamiento\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",              # dónde descargar\n",
    "    train=True,               # obtener datos de entrenamiento\n",
    "    download=True,            # descargar si no existe\n",
    "    transform=ToTensor(),     # convertir PIL a tensores\n",
    "    target_transform=None     # transformar labels (opcional)\n",
    ")\n",
    "\n",
    "# Descargar datos de prueba\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,              # obtener datos de prueba\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "```\n",
    "\n",
    "**ToTensor()**: Transforma imágenes PIL (formato estándar) a tensores de PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## 85. Formas de Entrada y Salida en Visión por Computadora\n",
    "\n",
    "### Estructura de una Imagen\n",
    "\n",
    "```python\n",
    "# Ver primera muestra\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape}\")  # torch.Size([1, 28, 28])\n",
    "print(f\"Label: {label}\")              # 9 (Ankle boot)\n",
    "```\n",
    "\n",
    "**Formato de imagen**: `[color_channels, height, width]`\n",
    "- **color_channels=1**: Escala de grises\n",
    "- **color_channels=3**: RGB (rojo, verde, azul)\n",
    "- **height=28**: Píxeles de alto\n",
    "- **width=28**: Píxeles de ancho\n",
    "\n",
    "### Orden de Dimensiones: CHW vs HWC\n",
    "\n",
    "PyTorch usa **NCHW** (channels first) como default:\n",
    "- **N**: Número de imágenes (batch size)\n",
    "- **C**: Canales de color\n",
    "- **H**: Altura\n",
    "- **W**: Ancho\n",
    "\n",
    "**Ejemplo con batch**:\n",
    "```python\n",
    "# Batch de 32 imágenes de FashionMNIST\n",
    "batch_shape = [32, 1, 28, 28]  # [batch_size, channels, height, width]\n",
    "```\n",
    "\n",
    "**Nota**: Algunos frameworks usan **NHWC** (channels last), que puede ser más eficiente en ciertos casos.\n",
    "\n",
    "---\n",
    "\n",
    "## 86. Visualización de Imágenes\n",
    "\n",
    "### Visualizar una Imagen\n",
    "\n",
    "```python\n",
    "image, label = train_data[0]\n",
    "class_names = train_data.classes\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")  # squeeze() elimina dimensión de canal\n",
    "plt.title(class_names[label])\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Concepto clave**:\n",
    "- `.squeeze()`: Elimina dimensiones de tamaño 1 (de [1, 28, 28] a [28, 28])\n",
    "- `cmap=\"gray\"`: Mapa de color para escala de grises\n",
    "\n",
    "### Visualizar Múltiples Imágenes\n",
    "\n",
    "```python\n",
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "rows, cols = 4, 4\n",
    "\n",
    "for i in range(1, rows * cols + 1):\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    \n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "```\n",
    "\n",
    "**Propósito**: Visualizar datos ayuda a entender patrones antes de construir el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## 87. DataLoader: Preparación de Datos por Batches\n",
    "\n",
    "### ¿Por qué usar Batches?\n",
    "\n",
    "Entrenar con **mini-batches** (porciones pequeñas de datos) en lugar del dataset completo es más eficiente:\n",
    "\n",
    "**Ventajas**:\n",
    "1. **Eficiencia computacional**: Procesar todo el dataset a la vez requiere memoria infinita\n",
    "2. **Más actualizaciones**: Gradient descent ocurre por batch, no por época\n",
    "3. **Mejora convergencia**: Actualizaciones más frecuentes pueden acelerar el aprendizaje\n",
    "\n",
    "**Batch size común**: 32 es un buen punto de partida (también 64, 128, 256)\n",
    "\n",
    "### Crear DataLoader\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# DataLoader de entrenamiento\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True  # mezclar datos cada época\n",
    ")\n",
    "\n",
    "# DataLoader de prueba\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False  # no es necesario mezclar datos de prueba\n",
    ")\n",
    "\n",
    "print(f\"Batches de entrenamiento: {len(train_dataloader)}\")\n",
    "print(f\"Batches de prueba: {len(test_dataloader)}\")\n",
    "```\n",
    "\n",
    "**Cálculo de batches**: 60,000 imágenes / 32 = 1,875 batches\n",
    "\n",
    "### Inspeccionar un Batch\n",
    "\n",
    "```python\n",
    "# Obtener primer batch\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features_batch.shape}\")  # [32, 1, 28, 28]\n",
    "print(f\"Labels batch shape: {train_labels_batch.shape}\")     # [32]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 88. Modelo Baseline con nn.Flatten\n",
    "\n",
    "### Arquitectura del Modelo\n",
    "\n",
    "```python\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),  # Aplanar imagen 2D a vector 1D\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "```\n",
    "\n",
    "### ¿Qué hace nn.Flatten()?\n",
    "\n",
    "```python\n",
    "# Ejemplo de flatten\n",
    "flatten_model = nn.Flatten()\n",
    "x = train_features_batch[0]  # [1, 28, 28]\n",
    "output = flatten_model(x)    # [1, 784]\n",
    "\n",
    "print(f\"Antes: {x.shape} -> [channels, height, width]\")\n",
    "print(f\"Después: {output.shape} -> [channels, height*width]\")\n",
    "```\n",
    "\n",
    "**Concepto**: Convierte matriz 2D (28×28=784) en vector 1D que `nn.Linear` puede procesar.\n",
    "\n",
    "### Instanciar el Modelo\n",
    "\n",
    "```python\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(\n",
    "    input_shape=784,              # 28*28 píxeles\n",
    "    hidden_units=10,              # unidades ocultas\n",
    "    output_shape=len(class_names) # 10 clases\n",
    ").to(\"cpu\")\n",
    "\n",
    "print(model_0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 89. Función de Pérdida y Optimizador para Clasificación Multi-clase\n",
    "\n",
    "### Configuración\n",
    "\n",
    "```python\n",
    "# Función de pérdida para multi-clase\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizador\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "**CrossEntropyLoss**:\n",
    "- Combina `nn.LogSoftmax()` y `nn.NLLLoss()`\n",
    "- Ideal para clasificación multi-clase\n",
    "- Trabaja directamente con logits (sin softmax manual)\n",
    "\n",
    "---\n",
    "\n",
    "## 90. Función de Precisión con TorchMetrics\n",
    "\n",
    "### Uso\n",
    "\n",
    "```python\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "# Crear métrica de precisión\n",
    "metric = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "# Calcular precisión\n",
    "metric.update(y_pred, y_true)\n",
    "accuracy = metric.compute()\n",
    "metric.reset()  # Reiniciar para siguiente época\n",
    "```\n",
    "\n",
    "**Parámetros**:\n",
    "- `task`: Tipo de problema (\"multiclass\", \"binary\", \"multilabel\")\n",
    "- `num_classes`: Número de clases en el problema\n",
    "\n",
    "---\n",
    "\n",
    "## 91. Training y Testing Loops para Visión por Computadora\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "```python\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "epochs = 3\n",
    "\n",
    "# Métrica de accuracy\n",
    "train_metric = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    \n",
    "    train_loss = 0\n",
    "    model_0.train()\n",
    "    \n",
    "    # Loop sobre batches de entrenamiento\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "        \n",
    "        # 2. Calcular pérdida\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_metric.update(y_pred, y)\n",
    "        \n",
    "        # 3. Limpiar gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Step del optimizador\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Imprimir progreso\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Visto {batch * len(X)}/{len(train_dataloader.dataset)} muestras\")\n",
    "    \n",
    "    # Calcular métricas promedio\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc = train_metric.compute()\n",
    "    train_metric.reset()\n",
    "    \n",
    "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.4f}\")\n",
    "```\n",
    "\n",
    "### Testing Loop\n",
    "\n",
    "```python\n",
    "# Métrica de test\n",
    "test_metric = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "model_0.eval()\n",
    "test_loss = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for X, y in test_dataloader:\n",
    "        # 1. Forward pass\n",
    "        test_pred = model_0(X)\n",
    "        \n",
    "        # 2. Calcular pérdida y accuracy\n",
    "        test_loss += loss_fn(test_pred, y)\n",
    "        test_metric.update(test_pred, y)\n",
    "    \n",
    "    # Calcular métricas promedio\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc = test_metric.compute()\n",
    "\n",
    "print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}%\")\n",
    "```\n",
    "\n",
    "**Conceptos clave**:\n",
    "- **train_loss / len(dataloader)**: Pérdida promedio por batch\n",
    "- **metric.compute()**: Calcular métrica acumulada\n",
    "- **metric.reset()**: Limpiar para siguiente época\n",
    "\n",
    "---\n",
    "\n",
    "## 92. Modelo Mejorado con No-Linealidad\n",
    "\n",
    "### Arquitectura V1\n",
    "\n",
    "```python\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),  # Activación no lineal\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Instanciar\n",
    "model_1 = FashionMNISTModelV1(\n",
    "    input_shape=784,\n",
    "    hidden_units=100,  # Más unidades ocultas\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "```\n",
    "\n",
    "**Mejoras**:\n",
    "- Añadir `nn.ReLU()` entre capas lineales\n",
    "- Incrementar `hidden_units` para mayor capacidad\n",
    "\n",
    "---\n",
    "\n",
    "## 93. Código Device-Agnostic (GPU/CPU)\n",
    "\n",
    "### Configuración del Dispositivo\n",
    "\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "```\n",
    "\n",
    "### Mover Datos y Modelo\n",
    "\n",
    "```python\n",
    "# Mover modelo al dispositivo\n",
    "model_1.to(device)\n",
    "\n",
    "# Mover datos en cada batch\n",
    "for X, y in train_dataloader:\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    # Entrenar...\n",
    "```\n",
    "\n",
    "**Importancia**: GPU acelera significativamente el entrenamiento de modelos grandes.\n",
    "\n",
    "---\n",
    "\n",
    "## 94. Redes Neuronales Convolucionales (CNN)\n",
    "\n",
    "### ¿Por qué CNN para Visión?\n",
    "\n",
    "Las CNN son especialmente diseñadas para procesar imágenes:\n",
    "\n",
    "**Ventajas**:\n",
    "1. **Parámetros compartidos**: Reducen número de pesos a aprender\n",
    "2. **Invariancia a traslación**: Detectan características sin importar posición\n",
    "3. **Jerarquía de características**: Aprenden de simple a complejo\n",
    "\n",
    "### Capas Clave\n",
    "\n",
    "| Capa | Función |\n",
    "|------|---------|\n",
    "| **Conv2d** | Extrae características locales aplicando filtros |\n",
    "| **ReLU** | Introduce no-linealidad |\n",
    "| **MaxPool2d** | Reduce dimensionalidad manteniendo características importantes |\n",
    "| **Flatten** | Convierte feature maps a vector 1D |\n",
    "| **Linear** | Clasificación final |\n",
    "\n",
    "### Arquitectura CNN Básica\n",
    "\n",
    "```python\n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                     out_channels=hidden_units,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                     out_channels=hidden_units,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                     out_channels=hidden_units,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                     out_channels=hidden_units,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7,\n",
    "                     out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### Parámetros de Conv2d\n",
    "\n",
    "```python\n",
    "nn.Conv2d(\n",
    "    in_channels=1,      # Canales de entrada (1 para escala de grises)\n",
    "    out_channels=10,    # Canales de salida (número de filtros)\n",
    "    kernel_size=3,      # Tamaño del filtro (3x3)\n",
    "    stride=1,           # Paso del filtro\n",
    "    padding=1           # Padding para mantener tamaño\n",
    ")\n",
    "```\n",
    "\n",
    "**Cálculo de salida**:\n",
    "$$\\text{output} = \\frac{\\text{input} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1$$\n",
    "\n",
    "---\n",
    "\n",
    "## 95. Comparación de Modelos\n",
    "\n",
    "### Evaluar Múltiples Modelos\n",
    "\n",
    "```python\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    loss, acc = 0, 0\n",
    "    metric = Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            metric.update(y_pred, y)\n",
    "        \n",
    "        loss /= len(data_loader)\n",
    "        acc = metric.compute()\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_loss\": loss.item(),\n",
    "        \"model_acc\": acc.item()\n",
    "    }\n",
    "\n",
    "# Comparar resultados\n",
    "model_0_results = eval_model(model_0, test_dataloader, loss_fn)\n",
    "model_1_results = eval_model(model_1, test_dataloader, loss_fn)\n",
    "model_2_results = eval_model(model_2, test_dataloader, loss_fn)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 96. Guardar y Cargar Modelos\n",
    "\n",
    "### Guardar Modelo\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"fashionmnist_model.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# Guardar state_dict\n",
    "torch.save(obj=model_2.state_dict(), f=MODEL_SAVE_PATH)\n",
    "```\n",
    "\n",
    "### Cargar Modelo\n",
    "\n",
    "```python\n",
    "# Crear nueva instancia\n",
    "loaded_model = FashionMNISTModelV2(\n",
    "    input_shape=1,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "# Cargar pesos guardados\n",
    "loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 97. Hacer Predicciones y Visualizar Resultados\n",
    "\n",
    "### Predicciones Aleatorias\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "def make_predictions(model, data, num_samples=9):\n",
    "    pred_probs = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for sample in random.sample(list(data), k=num_samples):\n",
    "            img, label = sample\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            \n",
    "            pred_logit = model(img)\n",
    "            pred_prob = torch.softmax(pred_logit, dim=1)\n",
    "            pred_probs.append(pred_prob.cpu())\n",
    "    \n",
    "    return torch.stack(pred_probs)\n",
    "```\n",
    "\n",
    "### Visualizar Predicciones\n",
    "\n",
    "```python\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "for sample, label in random.sample(list(test_data), k=9):\n",
    "    test_samples.append(sample)\n",
    "    test_labels.append(label)\n",
    "\n",
    "pred_probs = make_predictions(model_2, test_data)\n",
    "pred_classes = pred_probs.argmax(dim=1)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(9, 9))\n",
    "for i, sample in enumerate(test_samples):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
    "    \n",
    "    pred_label = class_names[pred_classes[i]]\n",
    "    true_label = class_names[test_labels[i]]\n",
    "    \n",
    "    title_color = \"g\" if pred_label == true_label else \"r\"\n",
    "    plt.title(f\"Pred: {pred_label}\\nTrue: {true_label}\", color=title_color)\n",
    "    plt.axis(False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 98. Matriz de Confusión\n",
    "\n",
    "### Crear Matriz de Confusión\n",
    "\n",
    "```python\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# Hacer predicciones en todo el conjunto de prueba\n",
    "y_preds = []\n",
    "model_2.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_logit = model_2(X)\n",
    "        y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n",
    "        y_preds.append(y_pred.cpu())\n",
    "\n",
    "y_pred_tensor = torch.cat(y_preds)\n",
    "\n",
    "# Calcular matriz de confusión\n",
    "confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\n",
    "confmat_tensor = confmat(preds=y_pred_tensor, target=test_data.targets)\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(),\n",
    "    class_names=class_names,\n",
    "    figsize=(10, 7)\n",
    ")\n",
    "```\n",
    "\n",
    "**Interpretación**:\n",
    "- **Diagonal**: Predicciones correctas\n",
    "- **Fuera de diagonal**: Errores de clasificación\n",
    "\n",
    "---\n",
    "\n",
    "## 99. Conceptos Clave de Visión por Computadora en PyTorch\n",
    "\n",
    "### Workflow de Visión por Computadora\n",
    "\n",
    "1. **Obtener datos**: Descargar dataset (FashionMNIST, CIFAR10)\n",
    "2. **Explorar datos**: Visualizar muestras y verificar formas\n",
    "3. **Preparar datos**: Crear DataLoaders con batch_size apropiado\n",
    "4. **Construir modelo**:\n",
    "   - Baseline: Capas lineales con Flatten\n",
    "   - Mejorado: Añadir ReLU\n",
    "   - Avanzado: CNN con Conv2d + MaxPool2d\n",
    "5. **Entrenar**: Loop de entrenamiento con pérdida y métricas\n",
    "6. **Evaluar**: Calcular accuracy, matriz de confusión\n",
    "7. **Mejorar**: Ajustar arquitectura e hiperparámetros\n",
    "8. **Guardar**: Guardar modelo entrenado\n",
    "\n",
    "### Diferencias entre Modelos\n",
    "\n",
    "| Característica | Baseline | Con ReLU | CNN |\n",
    "|----------------|----------|----------|-----|\n",
    "| **Capas** | Linear + Flatten | Linear + ReLU | Conv2d + MaxPool + Linear |\n",
    "| **Parámetros** | Muchos | Muchos | Menos (compartidos) |\n",
    "| **Rendimiento** | Bajo | Medio | Alto |\n",
    "| **Capacidad** | Limitada | Media | Alta |\n",
    "\n",
    "### Mejores Prácticas\n",
    "\n",
    "1. **Empezar simple**: Baseline primero\n",
    "2. **Visualizar datos**: Entender antes de modelar\n",
    "3. **Usar GPU**: Acelera entrenamiento significativamente\n",
    "4. **Monitorear métricas**: Pérdida y accuracy en train/test\n",
    "5. **Experimentar**: Probar diferentes arquitecturas\n",
    "6. **Validar resultados**: Matriz de confusión y visualización\n",
    "\n",
    "---\n",
    "\n",
    "## 100. Desafío Práctico: CIFAR10\n",
    "\n",
    "### Dataset CIFAR10\n",
    "\n",
    "**Características**:\n",
    "- **Imágenes**: 32×32 píxeles RGB (3 canales)\n",
    "- **Clases**: 10 (avión, auto, pájaro, gato, ciervo, perro, rana, caballo, barco, camión)\n",
    "- **Total**: 60,000 imágenes\n",
    "\n",
    "### Pasos del Desafío\n",
    "\n",
    "```python\n",
    "# 1. Cargar CIFAR10\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# 2. Crear DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 3. Construir modelo CNN\n",
    "class CIFAR10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),  # 3 canales de entrada (RGB)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# 4. Entrenar y evaluar\n",
    "model = CIFAR10Model().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ... (loop de entrenamiento similar a FashionMNIST)\n",
    "```\n",
    "\n",
    "**Diferencia clave**: CIFAR10 tiene 3 canales (RGB) vs 1 canal (escala de grises) en FashionMNIST.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 101. Convoluciones en PyTorch\n",
    "\n",
    "### Definición de Convolución\n",
    "La convolución es una operación fundamental en visión por computadora que extrae características de las imágenes. A diferencia de las capas densamente conectadas que aprenden patrones globales, las capas convolucionales aprenden patrones **locales** en ventanas pequeñas de la imagen.\n",
    "\n",
    "### Características Clave de las ConvNets\n",
    "\n",
    "#### 1. Invariancia a la Traslación\n",
    "Las ConvNets pueden reconocer un patrón sin importar dónde aparezca en la imagen.\n",
    "\n",
    "**Ventaja**: Si la red aprende un patrón en la esquina inferior derecha, puede reconocerlo en cualquier otra ubicación sin necesidad de reentrenamiento.\n",
    "\n",
    "#### 2. Jerarquías Espaciales de Patrones\n",
    "Las ConvNets aprenden características progresivamente más complejas:\n",
    "- **Primera capa convolucional**: Aprende patrones pequeños como bordes\n",
    "- **Segunda capa**: Aprende patrones más grandes usando características de la primera capa\n",
    "- **Capas subsecuentes**: Aprenden conceptos visuales cada vez más abstractos\n",
    "\n",
    "**Ejemplo**: Bordes → Texturas → Partes de objetos → Objetos completos\n",
    "\n",
    "---\n",
    "\n",
    "## 102. Anatomía de una Operación de Convolución\n",
    "\n",
    "### Feature Maps (Mapas de Características)\n",
    "Las convoluciones operan sobre tensores llamados **feature maps** con tres dimensiones:\n",
    "- **Altura** (height): Dimensión vertical\n",
    "- **Ancho** (width): Dimensión horizontal\n",
    "- **Profundidad** (depth/channels): Número de canales\n",
    "\n",
    "**Ejemplos**:\n",
    "- Imagen RGB: profundidad = 3 (rojo, verde, azul)\n",
    "- Imagen en escala de grises: profundidad = 1\n",
    "- Feature map interno: profundidad = número de filtros aplicados\n",
    "\n",
    "### Parámetros de una Capa Convolucional\n",
    "\n",
    "#### 1. Tamaño del Parche (Kernel Size)\n",
    "Define el tamaño de la ventana que se desliza sobre la imagen.\n",
    "\n",
    "**Valores comunes**: 3×3 o 5×5\n",
    "\n",
    "#### 2. Profundidad de Salida (Output Depth)\n",
    "Número de filtros que se aplican. Cada filtro aprende a detectar una característica diferente.\n",
    "\n",
    "**Concepto**: Un filtro podría detectar \"presencia de una cara\", otro \"presencia de texto\", etc.\n",
    "\n",
    "### Implementación en PyTorch\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Capa convolucional básica\n",
    "conv_layer = nn.Conv2d(\n",
    "    in_channels=1,      # Canales de entrada (1 para escala de grises)\n",
    "    out_channels=10,    # Número de filtros/características a aprender\n",
    "    kernel_size=3,      # Tamaño del filtro (3x3)\n",
    "    stride=1,           # Paso del filtro\n",
    "    padding=1           # Padding para mantener dimensiones\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 103. Padding en Convoluciones\n",
    "\n",
    "### Definición\n",
    "El **padding** consiste en agregar filas y columnas alrededor de los bordes del feature map de entrada para controlar las dimensiones espaciales de salida.\n",
    "\n",
    "### Tipos de Padding\n",
    "\n",
    "#### Valid Padding\n",
    "Sin padding. Solo se usan ubicaciones válidas donde el kernel cabe completamente.\n",
    "\n",
    "```python\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0)\n",
    "# o equivalentemente\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding='valid')\n",
    "```\n",
    "\n",
    "**Efecto**: La salida es más pequeña que la entrada.\n",
    "\n",
    "#### Same Padding\n",
    "Añade padding para que la salida tenga las mismas dimensiones espaciales que la entrada.\n",
    "\n",
    "```python\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding='same')\n",
    "# o manualmente para kernel 3x3\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "```\n",
    "\n",
    "**Cálculo de padding para same**:\n",
    "- Kernel 3×3: padding = 1\n",
    "- Kernel 5×5: padding = 2\n",
    "\n",
    "### Visualización de Padding\n",
    "\n",
    "```\n",
    "Input (5x5)          Con padding=1 (7x7)\n",
    "┌─────┐              ┌───────────┐\n",
    "│ X X │              │ 0 0 0 0 0 │\n",
    "│ X X │    →         │ 0 X X X 0 │\n",
    "│ X X │              │ 0 X X X 0 │\n",
    "└─────┘              │ 0 0 0 0 0 │\n",
    "                     └───────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 104. Strides (Paso del Filtro)\n",
    "\n",
    "### Definición\n",
    "El **stride** define la distancia entre dos ventanas de convolución consecutivas.\n",
    "\n",
    "**Valor por defecto**: 1 (ventanas contiguas)\n",
    "\n",
    "### Efecto del Stride\n",
    "\n",
    "```python\n",
    "# Stride = 1 (default)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1)\n",
    "\n",
    "# Stride = 2 (downsampling)\n",
    "conv2 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=2)\n",
    "```\n",
    "\n",
    "**Con stride=2**: \n",
    "- La salida tiene aproximadamente la mitad de las dimensiones espaciales de la entrada\n",
    "- Reduce el tamaño del feature map por un factor de 2\n",
    "\n",
    "**Nota**: Las convoluciones con stride > 1 son raras en modelos de clasificación. Para reducir dimensionalidad, se prefiere usar **max pooling**.\n",
    "\n",
    "---\n",
    "\n",
    "## 105. Max Pooling\n",
    "\n",
    "### Definición\n",
    "El **max pooling** extrae ventanas del feature map de entrada y devuelve el valor máximo de cada canal.\n",
    "\n",
    "**Conceptos clave**:\n",
    "- Similar a convolución, pero usa operación de `max` en lugar de transformación aprendida\n",
    "- Típicamente usa ventanas de 2×2 con stride=2\n",
    "- Reduce las dimensiones espaciales por un factor de 2\n",
    "\n",
    "### Implementación\n",
    "\n",
    "```python\n",
    "# Capa de max pooling\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# Aplicar a un tensor\n",
    "input_tensor = torch.randn(1, 64, 28, 28)  # [batch, channels, height, width]\n",
    "output = pool(input_tensor)\n",
    "print(output.shape)  # torch.Size([1, 64, 14, 14])\n",
    "```\n",
    "\n",
    "### Visualización\n",
    "\n",
    "```\n",
    "Input (4x4)          Max Pooling 2x2 (2x2)\n",
    "┌────────┐           ┌────┐\n",
    "│ 1  3 │ 2  4       │ 3  4│\n",
    "│ 5  6 │ 7  8   →   │ 8  9│\n",
    "│────────│           └────┘\n",
    "│ 2  1 │ 4  9\n",
    "│ 0  3 │ 5  2\n",
    "└────────┘\n",
    "```\n",
    "\n",
    "**Interpretación**: De cada región 2×2, se toma el valor máximo.\n",
    "\n",
    "---\n",
    "\n",
    "## 106. Arquitectura TinyVGG\n",
    "\n",
    "### Descripción\n",
    "TinyVGG es una versión simplificada de la arquitectura VGG, diseñada con propósitos educativos. Se compone de bloques repetidos de capas convolucionales seguidas de max pooling.\n",
    "\n",
    "### Estructura General\n",
    "\n",
    "```\n",
    "Input → [Conv → ReLU → Conv → ReLU → MaxPool] → [Conv → ReLU → Conv → ReLU → MaxPool] → Flatten → Linear → Output\n",
    "```\n",
    "\n",
    "### Implementación en PyTorch\n",
    "\n",
    "```python\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Bloque convolucional 1\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                     out_channels=hidden_units, \n",
    "                     kernel_size=3, \n",
    "                     stride=1, \n",
    "                     padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                     out_channels=hidden_units,\n",
    "                     kernel_size=3, \n",
    "                     stride=1, \n",
    "                     padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Bloque convolucional 2\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        # Clasificador\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7, \n",
    "                     out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### Cálculo de Dimensiones\n",
    "\n",
    "Para una imagen de entrada de 28×28:\n",
    "\n",
    "**Después del bloque 1**:\n",
    "- Conv (padding=1): 28×28 → 28×28\n",
    "- Conv (padding=1): 28×28 → 28×28\n",
    "- MaxPool (kernel=2): 28×28 → 14×14\n",
    "\n",
    "**Después del bloque 2**:\n",
    "- Conv (padding=1): 14×14 → 14×14\n",
    "- Conv (padding=1): 14×14 → 14×14\n",
    "- MaxPool (kernel=2): 14×14 → 7×7\n",
    "\n",
    "**Fórmula de salida**:\n",
    "$$\\text{Output} = \\left\\lfloor \\frac{I - K + 2P}{S} \\right\\rfloor + 1$$\n",
    "\n",
    "Donde:\n",
    "- $I$ = Tamaño de entrada\n",
    "- $K$ = Kernel size\n",
    "- $P$ = Padding\n",
    "- $S$ = Stride\n",
    "\n",
    "---\n",
    "\n",
    "## 107. Entrenamiento de CNN en FashionMNIST\n",
    "\n",
    "### Preparación del Modelo\n",
    "\n",
    "```python\n",
    "# Instanciar modelo\n",
    "model = TinyVGG(\n",
    "    input_shape=1,      # Escala de grises\n",
    "    hidden_units=10,    # Número de filtros\n",
    "    output_shape=10     # 10 clases\n",
    ").to(device)\n",
    "\n",
    "# Función de pérdida y optimizador\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "### Loop de Entrenamiento\n",
    "\n",
    "```python\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calcular pérdida promedio\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred = model(X)\n",
    "            test_loss += loss_fn(test_pred, y).item()\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    \n",
    "    print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Test loss: {test_loss:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 108. Desafío: AlexNet con CIFAR10\n",
    "\n",
    "### Arquitectura AlexNet\n",
    "\n",
    "AlexNet fue una de las primeras CNN profundas que ganó el concurso ImageNet en 2012.\n",
    "\n",
    "**Estructura**:\n",
    "- 5 capas convolucionales\n",
    "- 3 capas fully connected\n",
    "- ReLU como función de activación\n",
    "- Max pooling después de ciertas capas convolucionales\n",
    "\n",
    "### Implementación Simplificada\n",
    "\n",
    "```python\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: 3 → 96 canales\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2: 96 → 256 canales\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv3: 256 → 384 canales\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv4: 384 → 384 canales\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Conv5: 384 → 256 canales\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),  # Ajustar según dimensiones\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### Preparar Datos CIFAR10\n",
    "\n",
    "```python\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Transformaciones con Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # AlexNet espera 256x256\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Cargar datos\n",
    "train_data = datasets.CIFAR10(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "```\n",
    "\n",
    "### Entrenamiento\n",
    "\n",
    "```python\n",
    "# Instanciar modelo\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "\n",
    "# Optimizador y pérdida\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Entrenar (similar al loop anterior)\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Calcular métricas...\n",
    "        pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 109. Conceptos Clave de Redes Convolucionales\n",
    "\n",
    "### Ventajas de CNN sobre Redes Densas\n",
    "\n",
    "| Aspecto | Redes Densas | CNN |\n",
    "|---------|--------------|-----|\n",
    "| **Parámetros** | Muchos (todos conectados) | Menos (compartidos) |\n",
    "| **Invariancia espacial** | No | Sí |\n",
    "| **Aprendizaje de características** | Global | Local → Global (jerárquico) |\n",
    "| **Eficiencia** | Baja para imágenes | Alta |\n",
    "\n",
    "### Componentes Clave\n",
    "\n",
    "1. **Convolución (Conv2d)**: Extrae características locales\n",
    "2. **Activación (ReLU)**: Introduce no-linealidad\n",
    "3. **Pooling (MaxPool2d)**: Reduce dimensionalidad\n",
    "4. **Flatten**: Convierte feature maps a vector\n",
    "5. **Linear**: Clasificación final\n",
    "\n",
    "### Fórmula de Dimensiones\n",
    "\n",
    "Para calcular el tamaño de salida de una capa convolucional o pooling:\n",
    "\n",
    "$$\\text{Output\\_size} = \\left\\lfloor \\frac{\\text{Input\\_size} - \\text{Kernel\\_size} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right\\rfloor + 1$$\n",
    "\n",
    "**Ejemplo** (Input=28, Kernel=3, Padding=1, Stride=1):\n",
    "$$\\text{Output} = \\left\\lfloor \\frac{28 - 3 + 2 \\times 1}{1} \\right\\rfloor + 1 = 28$$\n",
    "\n",
    "### Mejores Prácticas\n",
    "\n",
    "1. **Empezar simple**: TinyVGG antes que arquitecturas complejas\n",
    "2. **Usar padding='same'**: Mantener dimensiones espaciales\n",
    "3. **MaxPooling después de bloques**: Reducir dimensionalidad gradualmente\n",
    "4. **Data Augmentation**: Rotar, voltear, recortar imágenes para generalización\n",
    "5. **Normalización**: Normalizar inputs para convergencia más rápida\n",
    "6. **Learning Rate**: Empezar con 0.01 o 0.001, ajustar según necesidad\n",
    "\n",
    "### Recursos Adicionales\n",
    "\n",
    "- **CNN Explainer**: https://poloclub.github.io/cnn-explainer/ (visualización interactiva)\n",
    "- **Papers**: VGG, ResNet, EfficientNet para arquitecturas avanzadas\n",
    "- **torchvision.models**: Modelos pre-entrenados listos para usar\n",
    "\n",
    "---\n",
    "\n",
    "## PyTorch Lightning\n",
    "\n",
    "### Introducción a PyTorch Lightning\n",
    "PyTorch Lightning es un marco de trabajo que simplifica el proceso de desarrollo de modelos de aprendizaje profundo en PyTorch. Su objetivo es eliminar el código repetitivo y permitir a los investigadores y desarrolladores centrarse en la lógica del modelo, facilitando la escalabilidad y la organización del código.\n",
    "\n",
    "\n",
    "### Estructura Básica de un Modelo\n",
    "Los modelos en PyTorch Lightning se construyen heredando de la clase `LightningModule`. Esta clase proporciona métodos de ciclo de vida que ayudan a estructurar el código de manera más clara.\n",
    "\n",
    "#### Métodos Clave\n",
    "1. **`forward`**: Define cómo se pasa la entrada a través del modelo.\n",
    "2. **`training_step`**: Contiene la lógica para un paso de entrenamiento, incluyendo la pérdida y las métricas.\n",
    "3. **`configure_optimizers`**: Define el optimizador que se utilizará para el entrenamiento.\n",
    "\n",
    "### Ejemplo de Modelo\n",
    "Aquí hay un ejemplo de cómo se puede definir un modelo simple en PyTorch Lightning:\n",
    "\n",
    "```python\n",
    "class SimpleModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "### Entrenamiento del Modelo\n",
    "Para entrenar el modelo, se utiliza la clase `Trainer`, que maneja el bucle de entrenamiento, la retropropagación y el registro de métricas.\n",
    "\n",
    "```python\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(model, train_dataloader)\n",
    "```\n",
    "\n",
    "### Predicciones y Evaluación\n",
    "Después de entrenar el modelo, se pueden hacer predicciones utilizando el método `eval()` y el contexto `torch.no_grad()` para desactivar el cálculo de gradientes, lo que ahorra memoria.\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data)\n",
    "```\n",
    "\n",
    "### Visualización de Resultados\n",
    "Es importante visualizar los resultados y las métricas de rendimiento. PyTorch Lightning permite registrar métricas que se pueden graficar para evaluar el rendimiento del modelo durante el entrenamiento.\n",
    "\n",
    "### Conclusiones\n",
    "PyTorch Lightning es una herramienta poderosa que ayuda a simplificar el proceso de desarrollo de modelos de aprendizaje profundo, permitiendo a los investigadores y desarrolladores centrarse en la lógica del modelo y la experimentación.\n",
    "\n",
    "### Recursos Adicionales\n",
    "- Documentación oficial de PyTorch Lightning: [PyTorch Lightning Docs](https://pytorch-lightning.readthedocs.io/en/stable/)\n",
    "- Ejemplos de modelos y tutoriales en el repositorio de GitHub de PyTorch Lightning.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "For:\n",
    "\n",
    "FashionMNISTModelCNN(\n",
    "\n",
    "  (block_1): Sequential(\n",
    "\n",
    "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), \n",
    "\n",
    "    padding=(1, 1))\n",
    "\n",
    "    (1): ReLU()\n",
    "\n",
    "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), \n",
    "\n",
    "    padding=(1, 1))\n",
    "\n",
    "    (3): ReLU()\n",
    "\n",
    "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "  )\n",
    "\n",
    "  (block_2): Sequential(\n",
    "\n",
    "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    (1): ReLU()\n",
    "\n",
    "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    (3): ReLU()\n",
    "\n",
    "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "  )\n",
    "\n",
    "  (classifier): Sequential(\n",
    "\n",
    "    (0): Flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
    "  \n",
    "  )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "The 7*7 comes from the spatial dimensions (Height x Width) of your data after it has passed through all the convolutional and pooling layers (block_1 and block_2).\n",
    "\n",
    "The nn.Flatten() layer's job is to take a multi-dimensional tensor and squash it into a 1D vector so it can be fed into a standard nn.Linear (fully connected) layer.\n",
    "\n",
    "Here’s a step-by-step trace of how the shape changes for FashionMNIST images, which are $28 \\times 28$ pixels.\n",
    "\n",
    "* Input: [batch_size, 1, 28, 28]\n",
    "* Pass through block_1:\n",
    "    * nn.Conv2d(..., padding=1): A $3 \\times 3$ kernel with padding=1 is \"same\" padding. It does not change the $28 \\times 28$ size.\n",
    "    * nn.ReLU(): Does not change the shape.\n",
    "    * nn.Conv2d(..., padding=1): Again, \"same\" padding. Shape remains $28 \\times 28$.\n",
    "    * nn.ReLU(): Does not change the shape.\n",
    "    * nn.MaxPool2d(kernel_size=2, stride=2): This layer halves the height and width.\n",
    "    * Output of block_1 Shape: [batch_size, hidden_units, 14, 14]\n",
    "* Pass through block_2:\n",
    "    * Input to block_2 is [batch_size, hidden_units, 14, 14].\n",
    "    * nn.Conv2d(..., padding=1): \"Same\" padding. Shape remains $14 \\times 14$.\n",
    "    * nn.ReLU(): Does not change the shape.\n",
    "    * nn.Conv2d(..., padding=1): \"Same\" padding. Shape remains $14 \\times 14$.\n",
    "    * nn.ReLU(): Does not change the shape.\n",
    "    * nn.MaxPool2d(2): This layer (with kernel size 2 and default stride 2) halves the height and width again.\n",
    "    * Output of block_2 Shape: [batch_size, hidden_units, 7, 7]\n",
    "* Pass to self.classifier:\n",
    "    * nn.Flatten(): This is the key. It takes the tensor from block_2 (shape [batch_size, hidden_units, 7, 7]) and \"flattens\" it. It keeps the batch dimension but multiplies all other dimensions together.\n",
    "    * Output of Flatten Shape: [batch_size, hidden_units * 7 * 7]\n",
    "    * nn.Linear(in_features=..., out_features=...): This layer requires a 1D vector (per item in the batch) as input. Its in_features must match the size of the vector it just received from nn.Flatten().\n",
    "    * Therefore, in_features must be hidden_units * 7 * 7.\n",
    "\n",
    "\n",
    "There is a standard formula to calculate the output height and width for both convolutional and pooling layers.The formula for the output dimension (Height or Width) is:$$\\text{Output} = \\lfloor \\frac{I - K + 2P}{S} \\rfloor + 1$$\n",
    "\n",
    "Where:\n",
    "* $I$ = Input dimension (e.g., input height $H_{in}$ or width $W_{in}$)\n",
    "* $K$ = Kernel size\n",
    "* $P$ = Padding\n",
    "* $S$ = Stride\n",
    "\n",
    "Applying the Formula to the model, let's trace the width (the calculation is identical for height) starting from the $28 \\times 28$ input.\n",
    "\n",
    "Block 1\n",
    "\n",
    "* First nn.Conv2d:\n",
    "    * Input ($I$) = 28\n",
    "    * Kernel ($K$) = 3\n",
    "    * Padding ($P$) = 1\n",
    "    * Stride ($S$) = 1\n",
    "    * Output = $\\lfloor \\frac{28 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor \\frac{27}{1} \\rfloor + 1 = 27 + 1 = \\mathbf{28}$\n",
    "* Second nn.Conv2d: (Input is 28 from the previous layer)\n",
    "    * Input ($I$) = 28\n",
    "    * Kernel ($K$) = 3\n",
    "    * Padding ($P$) = 1\n",
    "    * Stride ($S$) = 1\n",
    "    * Output = $\\lfloor \\frac{28 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor \\frac{27}{1} \\rfloor + 1 = 27 + 1 = \\mathbf{28}$\n",
    "* First nn.MaxPool2d: (Input is 28 from the previous layer)\n",
    "    * Input ($I$) = 28\n",
    "    * Kernel ($K$) = 2\n",
    "    * Padding ($P$) = 0 (default for MaxPool2d)\n",
    "    * Stride ($S$) = 2\n",
    "    * Output = $\\lfloor \\frac{28 - 2 + 2(0)}{2} \\rfloor + 1 = \\lfloor \\frac{26}{2} \\rfloor + 1 = 13 + 1 = \\mathbf{14}$\n",
    "* Output shape after block_1 is $14 \\times 14$.\n",
    "\n",
    "Block 2\n",
    "\n",
    "* Third nn.Conv2d: (Input is 14 from block_1)\n",
    "    * Input ($I$) = 14\n",
    "    * Kernel ($K$) = 3\n",
    "    * Padding ($P$) = 1\n",
    "    * Stride ($S$) = 1\n",
    "    * Output = $\\lfloor \\frac{14 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor \\frac{13}{1} \\rfloor + 1 = 13 + 1 = \\mathbf{14}$\n",
    "* Fourth nn.Conv2d: (Input is 14 from the previous layer)\n",
    "    * Input ($I$) = 14\n",
    "    * Kernel ($K$) = 3\n",
    "    * Padding ($P$) = 1\n",
    "    * Stride ($S$) = 1\n",
    "    * Output = $\\lfloor \\frac{14 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor \\frac{13}{1} \\rfloor + 1 = 13 + 1 = \\mathbf{14}$\n",
    "* Second nn.MaxPool2d: (Input is 14 from the previous layer)\n",
    "    * Input ($I$) = 14\n",
    "    * Kernel ($K$) = 2\n",
    "    * Padding ($P$) = 0 (default)\n",
    "    * Stride ($S$) = 2\n",
    "    * Output = $\\lfloor \\frac{14 - 2 + 2(0)}{2} \\rfloor + 1 = \\lfloor \\frac{12}{2} \\rfloor + 1 = 6 + 1 = \\mathbf{7}$\n",
    "* Final output shape after block_2 is $7 \\times 7$.\n",
    "\n",
    "This $7 \\times 7$ is then flattened (along with the hidden_units channels) to create the hidden_units * 7 * 7 vector for the linear layer.\n",
    "\n",
    "The 7*7 comes from the spatial dimensions (Height x Width) of your data after it has passed through all the convolutional and pooling layers (block_1 and block_2).\n",
    "\n",
    "The nn.Flatten() layer's job is to take a multi-dimensional tensor and squash it into a 1D vector so it can be fed into a standard nn.Linear (fully connected) layer.\n",
    "\n",
    "Here’s a step-by-step trace of how the shape changes for FashionMNIST images, which are $28 \\times 28$ pixels.\n",
    "\n",
    "* Input: [batch_size, 1, 28, 28]\n",
    "* Pass through block_1:\n",
    "    * nn.Conv2d(..., padding=1): A $3 \\times 3$ kernel with padding=1 is \"same\" padding. It does not change the $28 \\times 28$ size.\n",
    "    * nn.ReLU(): Does not change the shape.\n",
    "    * nn.Conv2d(..., padding=1): Again, \"same\" padding. Shape remains $28 \\times 28$.\n",
    "    * nn.ReLU(): Does not change the shape.\n",
    "    * nn.MaxPool2d(kernel_size=2, stride=2): This layer halves the height and width.\n",
    "    * Output of block_1 Shape: [batch_size, hidden_units, 14, 14]\n",
    "* Pass through block_2:\n",
    "    * Input to block_2 is [batch_size, hidden_units, 14, 14].\n",
    "    * nn.Conv2d(..., padding=1): \"Same\" padding. Shape remains $14 \\times 14$.\n",
    "    * nn.ReLU(): Does not change the shape.\n",
    "    * nn.Conv2d(..., padding=1): \"Same\" padding. Shape remains $14 \\times 14$.\n",
    "    * nn.ReLU(): Does not change the shape.\n",
    "    * nn.MaxPool2d(2): This layer (with kernel size 2 and default stride 2) halves the height and width again.\n",
    "    * Output of block_2 Shape: [batch_size, hidden_units, 7, 7]\n",
    "* Pass to self.classifier:\n",
    "    * nn.Flatten(): This is the key. It takes the tensor from block_2 (shape [batch_size, hidden_units, 7, 7]) and \"flattens\" it. It keeps the batch dimension but multiplies all other dimensions together.\n",
    "    * Output of Flatten Shape: [batch_size, hidden_units * 7 * 7]\n",
    "    * nn.Linear(in_features=..., out_features=...): This layer requires a 1D vector (per item in the batch) as input. Its in_features must match the size of the vector it just received from nn.Flatten().\n",
    "    * Therefore, in_features must be hidden_units * 7 * 7.\n",
    "\n",
    "\n",
    "There is a standard formula to calculate the output height and width for both convolutional and pooling layers.The formula for the output dimension (Height or Width) is:$$\\text{Output} = \\lfloor \\frac{I - K + 2P}{S} \\rfloor + 1$$\n",
    "\n",
    "Where:\n",
    "* $I$ = Input dimension (e.g., input height $H_{in}$ or width $W_{in}$)\n",
    "* $K$ = Kernel size\n",
    "* $P$ = Padding\n",
    "* $S$ = Stride\n",
    "\n",
    "Applying the Formula to the model, let's trace the width (the calculation is identical for height) starting from the $28 \\times 28$ input.\n",
    "\n",
    "Block 1\n",
    "\n",
    "* First nn.Conv2d:\n",
    "    * Input ($I$) = 28\n",
    "    * Kernel ($K$) = 3\n",
    "    * Padding ($P$) = 1\n",
    "    * Stride ($S$) = 1\n",
    "    * Output = $\\lfloor \\frac{28 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor \\frac{27}{1} \\rfloor + 1 = 27 + 1 = \\mathbf{28}$\n",
    "* Second nn.Conv2d: (Input is 28 from the previous layer)\n",
    "    * Input ($I$) = 28\n",
    "    * Kernel ($K$) = 3\n",
    "    * Padding ($P$) = 1\n",
    "    * Stride ($S$) = 1\n",
    "    * Output = $\\lfloor \\frac{28 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor \\frac{27}{1} \\rfloor + 1 = 27 + 1 = \\mathbf{28}$\n",
    "* First nn.MaxPool2d: (Input is 28 from the previous layer)\n",
    "    * Input ($I$) = 28\n",
    "    * Kernel ($K$) = 2\n",
    "    * Padding ($P$) = 0 (default for MaxPool2d)\n",
    "    * Stride ($S$) = 2\n",
    "    * Output = $\\lfloor \\frac{28 - 2 + 2(0)}{2} \\rfloor + 1 = \\lfloor \\frac{26}{2} \\rfloor + 1 = 13 + 1 = \\mathbf{14}$\n",
    "* Output shape after block_1 is $14 \\times 14$.\n",
    "\n",
    "Block 2\n",
    "\n",
    "* Third nn.Conv2d: (Input is 14 from block_1)\n",
    "    * Input ($I$) = 14\n",
    "    * Kernel ($K$) = 3\n",
    "    * Padding ($P$) = 1\n",
    "    * Stride ($S$) = 1\n",
    "    * Output = $\\lfloor \\frac{14 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor \\frac{13}{1} \\rfloor + 1 = 13 + 1 = \\mathbf{14}$\n",
    "* Fourth nn.Conv2d: (Input is 14 from the previous layer)\n",
    "    * Input ($I$) = 14\n",
    "    * Kernel ($K$) = 3\n",
    "    * Padding ($P$) = 1\n",
    "    * Stride ($S$) = 1\n",
    "    * Output = $\\lfloor \\frac{14 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor \\frac{13}{1} \\rfloor + 1 = 13 + 1 = \\mathbf{14}$\n",
    "* Second nn.MaxPool2d: (Input is 14 from the previous layer)\n",
    "    * Input ($I$) = 14\n",
    "    * Kernel ($K$) = 2\n",
    "    * Padding ($P$) = 0 (default)\n",
    "    * Stride ($S$) = 2\n",
    "    * Output = $\\lfloor \\frac{14 - 2 + 2(0)}{2} \\rfloor + 1 = \\lfloor \\frac{12}{2} \\rfloor + 1 = 6 + 1 = \\mathbf{7}$\n",
    "* Final output shape after block_2 is $7 \\times 7$.\n",
    "\n",
    "This $7 \\times 7$ is then flattened (along with the hidden_units channels) to create the hidden_units * 7 * 7 vector for the linear layer."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
